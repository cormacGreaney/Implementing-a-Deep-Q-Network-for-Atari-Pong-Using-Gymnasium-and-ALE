{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC4hBwSrapqkLKuPVnLLbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cormacGreaney/Implementing-a-Deep-Q-Network-for-Atari-Pong-Using-Gymnasium-and-ALE/blob/Cube-Testing-Fun-Branch/trying%20to%20build%20a%20modek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rCD_DF9osKG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym            # main RL library\n",
        "import numpy as np                 # handy for arrays\n",
        "import matplotlib.pyplot as plt    # for visuals\n",
        "import ale_py                      # Atari emulator backend\n",
        "\n",
        "# Plug the Atari environments into Gymnasium.\n",
        "# Without this, Pong won't appear in the registry.\n",
        "gym.register_envs(ale_py)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a Pong environment that returns RGB frames so we can display them.\n",
        "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "# gets image information\n",
        "height, width, channels = env.observation_space.shape\n",
        "\n",
        "# gets all available actions\n",
        "num_actions = env.action_space.sample()\n",
        "\n",
        "# Reset the environment to start a new game.\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Just print what kind of data we got back.\n",
        "print(\"Obs type/shape:\", type(obs), getattr(obs, \"shape\", None))"
      ],
      "metadata": {
        "id": "s3v3Tf4eowJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "metadata": {
        "id": "AtYPSNv6YG2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll collect a few frames by taking random actions.\n",
        "terminated = False\n",
        "truncated = False\n",
        "frames = []\n",
        "\n",
        "for t in range(200):\n",
        "    # Choose a random action from Pong's action space.\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Step the environment forward.\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Save the frame so we can preview it later.\n",
        "    frames.append(obs)\n",
        "\n",
        "    # Stop early if the episode ends.\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "print(\"Collected\", len(frames), \"frames\")"
      ],
      "metadata": {
        "id": "8TAF5KcQox1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the very first frame we grabbed.\n",
        "plt.imshow(frames[0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3-zuM9yrozFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# will run 5 different games of pong\n",
        "episodes = 5\n",
        "for episode in range(1, episodes + 1):\n",
        "  # resets the env space\n",
        "    obs, info = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    score = 0\n",
        "\n",
        "    # while the instance is not terminated it will run random actions to see its performance\n",
        "    while not terminated and not truncated:\n",
        "\n",
        "      # env.render() # Uncomment to see the rendering, idk where it renders\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "      # after its random actions and is terminated will return its rewards\n",
        "      obs, reward, terminated, truncated, info= env.step(action)\n",
        "      score += reward\n",
        "\n",
        "      # print out episodes and there scores\n",
        "    print('Episode:{} Score{}'.format(episode, score))\n",
        "env.close()"
      ],
      "metadata": {
        "id": "w3QMSEJEY7Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "kfvfCr7peakM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the shape of the screen will define hopw out deep learning model looks like\n",
        "def build_model(height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "\n",
        "    # as we use a image based model we will use convelution to flatten the image, we will train filters to detect where the agients are in the images\n",
        "    model.add(Conv2D(31, (8,8), strides=(4,4), activation='relu', input_shape=(height, width, channels)))\n",
        "\n",
        "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "    # idk yet what dis do\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "\n",
        "    # compresses based on the number of actions that pong can make\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ulpZE4ABepWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model :)\n",
        "model = build_model(height, width, channels, num_actions)"
      ],
      "metadata": {
        "id": "nf252n0ihS7I",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "X2et_pElhdWH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras_rl2.agents import DQNAgent\n",
        "from keras_rl2.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from keras_rl2.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "CAEAq0LZno8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "  policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
        "  memory = SequentialMemory(limit=1000, window_length=3)\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, enable_dueling_network=True, dueling_type='avg', nb_actions=actions, nb_steps_warmup=1000)\n",
        "  return dqn"
      ],
      "metadata": {
        "id": "hPk-ECDbkd2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model,action)\n",
        "dqn.compile(Adam(learning_rate=1e-4))\n",
        "dqn.fit(env, nb_steps=10000,visualize = False,verbose=2)"
      ],
      "metadata": {
        "id": "90l59urSmfAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}