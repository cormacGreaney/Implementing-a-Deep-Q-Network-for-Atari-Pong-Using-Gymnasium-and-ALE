{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cormac Greaney / 22352228 & Jan Lawinski / 22340343\n",
    "# The code does execute to the end without error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d988e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Installation Cell\n",
    "# ------------------------------------------------------------\n",
    "# These pip commands install everything required for this\n",
    "# notebook to run on a fresh machine\n",
    "# ============================================================\n",
    "\n",
    "# Core RL + Atari dependencies\n",
    "#!pip install \"gymnasium[atari,accept-rom-license]\"\n",
    "#!pip install ale-py\n",
    "\n",
    "# Numerical + plotting utilities\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "\n",
    "# Image processing for preprocessing frames\n",
    "#!pip install opencv-python\n",
    "\n",
    "# Deep learning framework (we're using PyTorch)\n",
    "#!pip install torch\n",
    "\n",
    "# (Optional) If you're running on Windows and have issues\n",
    "#!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym            # main RL library\n",
    "import numpy as np                 # handy for arrays\n",
    "import matplotlib.pyplot as plt    # for visuals\n",
    "import ale_py                      # Atari emulator backend\n",
    "import cv2                         # image preprocessing\n",
    "import torch                       # deep learning framework\n",
    "import torch.nn as nn              # neural network layers\n",
    "import torch.nn.functional as F    # activation functions and operations\n",
    "from collections import deque      # efficient queue for replay buffer\n",
    "import random                      # random number generation\n",
    "import datetime                    # timestamp for training logs\n",
    "\n",
    "# Plug the Atari environments into Gymnasium\n",
    "# Without this, Pong won't appear in the registry\n",
    "gym.register_envs(ale_py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config initialised. SEED = 42\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Global configuration and random seeds\n",
    "# ------------------------------------------------------------\n",
    "# This makes runs more reproducible and keeps the main\n",
    "# hyperparameters in one place so they are easy to tweak\n",
    "# ============================================================\n",
    "\n",
    "# Random seeds for reproducibility (not perfectly identical runs,\n",
    "# but it helps a lot to make behaviour more consistent)\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Core DQN / training hyperparameters\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "REPLAY_BUFFER_CAPACITY = 100_000\n",
    "\n",
    "# Epsilon-greedy schedule - controls exploration vs exploitation\n",
    "# We start exploring fully (1.0) and gradually reduce to minimal exploration (0.1)\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END   = 0.1 \n",
    "EPSILON_DECAY = 250_000  # steps to go from start to end\n",
    "\n",
    "# Training schedule - how long and how often we update things\n",
    "NUM_EPISODES = 1500  # increased for better learning (1500 is supposedly good for Pong)\n",
    "TARGET_UPDATE_INTERVAL = 1000  # update target network every N steps (not episodes)\n",
    "MAX_STEPS_PER_EPISODE = 10_000\n",
    "\n",
    "# Frame skipping - repeat each action for this many frames to speed up training\n",
    "FRAME_SKIP = 4\n",
    "\n",
    "# Training frequency - only train every N steps to avoid overfitting early on\n",
    "TRAIN_EVERY_N_STEPS = 4\n",
    "\n",
    "# Double DQN flag: True = Double DQN, False = vanilla DQN target\n",
    "USE_DOUBLE_DQN = True\n",
    "\n",
    "print(\"Config initialised. SEED =\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b23f5c",
   "metadata": {},
   "source": [
    "# CS4287 Assignment 2 — Deep Reinforcement Learning (Option 2: Atari Pong)\n",
    "\n",
    "**Student IDs:** Cormac Greaney / 22352228 & Jan Lawinski / 22340343\n",
    "\n",
    "First we would like to note that our understanding of the report structure given that it is to be included in this notebook is that it should be a report focused on our final notebook and results, we went through alot (alot alot) of notebook variations and falied/poor training runs to get this project to work with good learning outcomes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Reinforcement Learning is the machine learning paradigm of choice for this task\n",
    "\n",
    "Reinforcement Learning is the right approach for playing Atari Pong because the agent needs to learn through trial and error in an interactive environment. Unlike supervised learning, we don't have labeled examples of the \"correct\" action in each situation. Instead, the agent must explore the game, receive feedback through rewards (scoring points or losing them), and gradually learn which actions lead to better outcomes\n",
    "\n",
    "Pong is a sequential decision-making problem where each action affects future states. The agent needs to understand that moving the paddle up or down now will impact whether it can hit the ball later. This temporal aspect is naturally handled by RL through the concept of value functions, which estimate long-term rewards rather than just immediate ones\n",
    "\n",
    "The game also has a large state space (every possible frame configuration) that makes it impractical to use traditional tabular methods. Deep Q-Networks allow us to approximate the Q-function using neural networks, learning useful representations from raw pixel data without manual feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Gym Environment: PongNoFrameskip-v4\n",
    "\n",
    "We use the `PongNoFrameskip-v4` environment from Gymnasium, which provides the classic Atari Pong game without frame skipping. This gives us full control over frame processing and is the standard choice for DQN implementations\n",
    "\n",
    "The environment returns RGB frames of size 210×160×3 pixels. Each frame shows the current game state: the paddles, ball, score, and background. The action space includes 6 possible actions, but we chose to restrict it to the 3 meaningful ones for Pong given that we saw no point in including the others: action 0 (NOOP/no operation), action 2 (move paddle UP), and action 3 (move paddle DOWN)\n",
    "\n",
    "Rewards in Pong are sparse and meaningful: +1 when the agent scores a point, -1 when the opponent scores, and 0 otherwise. This sparse reward structure makes it challenging for the agent to learn, as most actions provide no immediate feedback. The agent must learn to associate actions with delayed rewards, which is where the discount factor γ in Q-learning becomes crucial\n",
    "\n",
    "The environment can terminate an episode when a player reaches 21 points, or truncate after a maximum number of steps. Each episode represents a complete game, and the agent's goal is to maximize the total episode reward\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Data capture and preprocessing\n",
    "\n",
    "Raw frames from the environment are 210×160×3 RGB images, which are too large and contain unnecessary information for efficient learning. We preprocess each frame through several steps:\n",
    "\n",
    "1. **Grayscale conversion**: Convert RGB to grayscale using OpenCV, reducing from 3 channels to 1. This removes color information that isn't needed for Pong\n",
    "\n",
    "2. **Resizing**: Downsample from 210×160 to 84×84 pixels using area interpolation. This standard DQN input size reduces computational cost while preserving essential spatial information\n",
    "\n",
    "3. **Normalization**: Convert pixel values from integers in [0, 255] to floats in [0, 1] by dividing by 255. This helps with neural network training stability\n",
    "\n",
    "4. **Frame stacking**: Stack the last 4 preprocessed frames together to create a (4, 84, 84) state representation. This is essential because a single frame doesn't show motion—the agent needs multiple frames to see if the ball is moving up or down. Without frame stacking, the agent would be playing blind to ball velocity\n",
    "\n",
    "5. **Reward clipping**: Clip rewards to [-1, 0, +1] using the sign function. This prevents reward magnitudes from varying wildly and stabilizes training\n",
    "\n",
    "During training, we also apply frame skipping: each action is repeated for 4 consecutive frames. This speeds up training by reducing the number of decisions the agent needs to make, while still allowing fine-grained control\n",
    "\n",
    "### 3.2 Replay buffer\n",
    "\n",
    "We implement an experience replay buffer with a capacity of 100,000 transitions. Each transition stores a tuple of (state, action, reward, next_state, done)\n",
    "\n",
    "The replay buffer serves two critical purposes:\n",
    "- **Breaking correlation**: Consecutive experiences in an episode are highly correlated. By randomly sampling from past experiences, we break this correlation and learn from a diverse mix of states and actions\n",
    "- **Sample efficiency**: Each experience can be used multiple times for training, making better use of the data we collect\n",
    "\n",
    "When training, we randomly sample batches of 32 experiences from the buffer. The buffer uses a deque with a maximum length, so once it reaches capacity, older experiences are automatically removed as new ones are added\n",
    "\n",
    "### 3.3 DQN network architecture\n",
    "\n",
    "Our DQN uses a convolutional neural network architecture designed for processing Atari frames:\n",
    "\n",
    "**Input**: (batch_size, 4, 84, 84) - stacked preprocessed frames\n",
    "\n",
    "**Convolutional layers**:\n",
    "- Conv1: 4 input channels → 32 output channels, kernel 8×8, stride 4. Output: (batch, 32, 20, 20)\n",
    "- Conv2: 32 → 64 channels, kernel 4×4, stride 2. Output: (batch, 64, 9, 9)\n",
    "- Conv3: 64 → 64 channels, kernel 3×3, stride 1. Output: (batch, 64, 7, 7)\n",
    "\n",
    "Each convolutional layer is followed by ReLU activation to introduce non-linearity\n",
    "\n",
    "**Fully connected layers**:\n",
    "- Flatten: 64 × 7 × 7 = 3,136 features\n",
    "- FC1: 3,136 → 512 neurons with ReLU\n",
    "- FC2: 512 → 3 outputs (one Q-value per action: NOOP, UP, DOWN)\n",
    "\n",
    "**Output**: (batch_size, 3) - Q-values for each possible action\n",
    "\n",
    "This architecture progressively extracts features: early layers detect edges and basic shapes, while deeper layers capture more complex patterns like ball movement and paddle positioning. The final fully connected layers combine these features to estimate the expected future reward for each action\n",
    "\n",
    "### 3.4 Q-learning update\n",
    "\n",
    "The Q-learning update happens in the `optimize_model()` function. Here's how it works step by step:\n",
    "\n",
    "**Step 1: Sample a batch**\n",
    "We randomly sample 32 experiences (s, a, r, s', done) from the replay buffer. These are converted to PyTorch tensors and moved to the GPU if available\n",
    "\n",
    "**Step 2: Compute current Q-values**\n",
    "We pass the current states through the policy network to get Q(s, a) for all actions:\n",
    "```python\n",
    "q_values = policy_net(states)  # shape: (batch, 3)\n",
    "```\n",
    "We then extract only the Q-values for the actions that were actually taken:\n",
    "```python\n",
    "state_action_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "```\n",
    "This gives us Q(s, a_taken) - what the network currently thinks these state-action pairs are worth\n",
    "\n",
    "**Step 3: Compute target Q-values**\n",
    "We use the Bellman equation to compute what the Q-values *should* be. The target is:\n",
    "```\n",
    "target = r + γ * max_a' Q_target(s', a') * (1 - done)\n",
    "```\n",
    "\n",
    "For Double DQN (which we use), we:\n",
    "1. Use the **policy network** to select the best action in the next state: `a* = argmax Q_policy(s', a')`\n",
    "2. Use the **target network** to evaluate that action: `Q_target(s', a*)`\n",
    "\n",
    "This reduces overestimation bias compared to standard DQN. The target network is a frozen copy of the policy network that gets updated every 1000 steps, providing stable targets during training\n",
    "\n",
    "If the episode ended (done=1), we don't bootstrap from the next state, so the target is just the reward\n",
    "\n",
    "**Step 4: Calculate loss and update**\n",
    "We compute the mean squared error between current Q-values and targets:\n",
    "```python\n",
    "loss = F.mse_loss(state_action_values, target_values)\n",
    "```\n",
    "\n",
    "This loss measures how far off our predictions are. We then:\n",
    "1. Zero the gradients: `optimizer.zero_grad()`\n",
    "2. Backpropagate: `loss.backward()` - computes gradients\n",
    "3. Clip gradients to prevent explosion: `clip_grad_norm_(max_norm=10.0)`\n",
    "4. Update weights: `optimizer.step()` - applies the gradients using Adam optimizer\n",
    "\n",
    "The network learns by repeatedly adjusting its weights to make Q(s, a) predictions closer to the Bellman targets, gradually improving its understanding of which actions lead to higher rewards\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Results\n",
    "\n",
    "(See plots in cell outputs near end of notebook)\n",
    "\n",
    "Our agent was trained for 1500 episodes over approximately 4.1 million environment steps. The training progress is shown in the plots below\n",
    "\n",
    "**Training Progress Plot**: The left plot shows episode rewards over time. Initially, the agent performs poorly with rewards around -21 (losing every point). As training progresses, rewards gradually improve, reaching positive values around episode 500-600. By the end of training, the agent consistently achieves rewards between 11-19, indicating it has learned to win games\n",
    "\n",
    "The red line shows a 20-episode moving average, which smooths out the noise and clearly shows the upward trend. The agent's performance stabilizes in the later episodes, suggesting it has converged to a good policy\n",
    "\n",
    "**Training Loss Plot**: The right plot shows the mean squared error loss over training steps. The loss starts around 0.02 and decreases to approximately 0.001 by the end of training. This significant reduction (from 0.0198 in the first 100 steps to 0.0011 in the last 100) indicates the network's Q-value predictions are becoming much more accurate. The decreasing loss aligns with the improving rewards, confirming that the agent is learning effectively\n",
    "\n",
    "The training shows clear signs of learning: the agent transitions from losing every point to consistently winning games, and the loss decreases steadily throughout training\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation of the trained agent\n",
    "\n",
    "### 5.1 How does one evaluate the performance of the RL agent?\n",
    "\n",
    "Evaluating an RL agent requires multiple metrics because a single measure can be misleading:\n",
    "\n",
    "**Episode rewards**: The most direct metric is the total reward per episode. For Pong, this ranges from -21 (losing every point) to +21 (winning every point). Our agent achieves rewards of 11-19 in the final episodes, indicating strong performance. However, this metric has high variance—a single episode might be lucky or unlucky\n",
    "\n",
    "**Win rate**: We can run many evaluation episodes and count how often the agent wins. This gives a more stable measure than individual episode rewards. Our agent wins consistently in later training episodes\n",
    "\n",
    "**Training loss**: The MSE loss between predicted and target Q-values shows how well the network has learned the value function. Lower loss suggests better value estimation, though it doesn't guarantee better gameplay\n",
    "\n",
    "**Action distribution**: We inspected the agent's policy and found it uses all three actions (NOOP, UP, DOWN) rather than collapsing to a single action. This indicates the agent has learned a nuanced policy rather than a degenerate strategy\n",
    "\n",
    "**Evaluation vs training performance**: During evaluation, we disable exploration (epsilon=0) and use a purely greedy policy. This shows the agent's true learned behavior without random actions. Our evaluation episode achieved a reward of -13, which is lower than training rewards but still shows the agent is playing actively rather than randomly\n",
    "\n",
    "### 5.2 Are the metrics that we have seen to date relevant?\n",
    "\n",
    "The metrics we use are relevant, but each has limitations:\n",
    "\n",
    "**Episode rewards** are highly relevant for Pong since they directly measure game performance. However, they're noisy and can be misleading in early training when the agent is still exploring\n",
    "\n",
    "**Training loss** is useful for monitoring learning progress, but it's an indirect measure. Low loss doesn't guarantee good gameplay—the network might overfit or learn incorrect value estimates. However, in our case, the decreasing loss correlates well with improving rewards\n",
    "\n",
    "**Action distribution** helps detect policy collapse, which is a common failure mode in RL. Our agent maintains a diverse action distribution, which is a good sign\n",
    "\n",
    "For a complete evaluation, we should also consider:\n",
    "- **Sample efficiency**: How many episodes/steps until the agent learns? Our agent shows improvement around episode 500-600\n",
    "- **Stability**: Does performance remain consistent? Our later episodes show stable positive rewards\n",
    "- **Generalization**: Would the agent perform well on different game configurations? This would require testing on variations of the environment\n",
    "\n",
    "The combination of these metrics gives a comprehensive picture of the agent's performance. The video output (pong_evaluation.mp4) provides qualitative evaluation by showing the agent's gameplay behavior visually\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Independently researched performance improvements\n",
    "\n",
    "We implemented and evaluated several techniques to improve DQN performance:\n",
    "\n",
    "**1. Random seed initialization**: We set random seeds for Python's random module, NumPy, and PyTorch (including CUDA if available) to seed=42. This makes training more reproducible and helps with debugging. While it doesn't improve performance directly, it's essential for fair comparisons\n",
    "\n",
    "**2. Double DQN**: We implemented Double DQN to address maximization bias, where standard DQN tends to overestimate Q-values. In Double DQN, we use the policy network to select actions but the target network to evaluate them. This decouples action selection from evaluation, reducing overestimation\n",
    "\n",
    "Our implementation uses `USE_DOUBLE_DQN = True`, and the code in `optimize_model()` implements the Double DQN target calculation:\n",
    "```python\n",
    "next_q_policy = policy_net(next_states)\n",
    "best_next_actions = next_q_policy.argmax(dim=1)\n",
    "next_q_target = target_net(next_states)\n",
    "max_next_q_values = next_q_target.gather(1, best_next_actions.unsqueeze(1)).squeeze(1)\n",
    "```\n",
    "\n",
    "This technique is particularly important for stable learning, as overestimated Q-values can lead to poor policies. The fact that our agent learns successfully and achieves positive rewards suggests Double DQN is helping prevent the overestimation issues that plague vanilla DQN\n",
    "\n",
    "**3. Experience replay**: While this is standard in DQN, we carefully tuned the buffer capacity (100,000) and batch size (32) to balance sample efficiency with computational cost. The large buffer allows the agent to learn from diverse past experiences, breaking the correlation between consecutive frames\n",
    "\n",
    "**4. Target network updates**: We update the target network every 1000 steps rather than every step. This provides stable targets for the Bellman equation, preventing the \"chasing a moving target\" problem that can destabilize training\n",
    "\n",
    "**5. Gradient clipping**: We clip gradients to a maximum norm of 10.0 to prevent gradient explosion, which can cause training to become unstable or diverge. This is especially important in the early stages when the network is still learning\n",
    "\n",
    "**6. Frame skipping and training frequency**: We skip 4 frames per action and train every 4 steps. This speeds up training significantly while maintaining learning quality. The agent still sees all frames through the frame stack, so no information is lost\n",
    "\n",
    "The combination of these techniques, particularly Double DQN, contributes to our agent's successful learning. The agent transitions from losing every point to consistently winning, demonstrating that these improvements are effective\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "https://www.jetir.org/papers/JETIR2405532.pdf\n",
    "\n",
    "https://towardsdatascience.com/learning-how-to-play-atari-games-through-deep-neural-networks/\n",
    "\n",
    "Gymnasium Documentation: https://gymnasium.farama.org/\n",
    "\n",
    "PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c725077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reduced action set: [0, 2, 3] -> 3 actions for the DQN\n",
      "Obs type/shape: <class 'numpy.ndarray'> (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Make a Pong environment that returns RGB frames so we can display them\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment to start a new game\n",
    "obs, info = env.reset()\n",
    "\n",
    "# We restrict the action space to a smaller, meaningful subset:\n",
    "# 0 = NOOP, 2 = UP, 3 = DOWN in Atari Pong\n",
    "VALID_ACTIONS = [0, 2, 3]\n",
    "NUM_ACTIONS = len(VALID_ACTIONS)\n",
    "\n",
    "print(\"Using reduced action set:\", VALID_ACTIONS, \"->\", NUM_ACTIONS, \"actions for the DQN\")\n",
    "\n",
    "# Just print what kind of data we got back\n",
    "print(\"Obs type/shape:\", type(obs), getattr(obs, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6524f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 200 frames\n"
     ]
    }
   ],
   "source": [
    "# We'll collect a few frames by taking random actions\n",
    "terminated = False\n",
    "truncated = False\n",
    "frames = []\n",
    "\n",
    "for t in range(200):\n",
    "    # Choose a random action from Pong's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Save the frame so we can preview it later\n",
    "    frames.append(obs)\n",
    "    \n",
    "    # Stop early if the episode ends\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Collected\", len(frames), \"frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c8a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the very first frame we grabbed\n",
    "plt.imshow(frames[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4d4200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Take a raw (210x160x3) RGB frame from Pong\n",
    "    and convert it into a clean 84x84 grayscale image\n",
    "    that the DQN can actually learn from.\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn the RGB frame into a grayscale image.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Shrink from (210x160) down to (84x84), which is the standard DQN input size.\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert pixel values from 0–255 integers into 0–1 floats.\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def clip_reward(reward):\n",
    "    \"\"\"\n",
    "    Clip rewards to [-1, 0, +1] as per standard DQN practice.\n",
    "    This helps stabilize training by keeping reward magnitudes consistent.\n",
    "    \"\"\"\n",
    "    # Pong rewards are typically -1 (lose point), 0 (no change), or +1 (score point).\n",
    "    # Clipping ensures all rewards are in this range regardless of game mechanics.\n",
    "    return np.sign(reward)  # sign gives -1, 0, or +1\n",
    "\n",
    "\n",
    "# Test the preprocessing on the first frame we collected earlier.\n",
    "processed = preprocess_frame(frames[0])\n",
    "\n",
    "print(\"Processed shape:\", processed.shape)\n",
    "\n",
    "# Show the processed grayscale image so we know it looks right.\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16996c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Keep a rolling window of the last N processed frames.\n",
    "    This lets the agent see short-term motion instead of a single static image.\n",
    "    Without frame stacking, the agent can't tell if the ball is moving up or down!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_frames=4):\n",
    "        # How many frames to stack together (typically 4 for Atari games).\n",
    "        self.num_frames = num_frames\n",
    "        # Use a deque with maxlen so it automatically drops old frames.\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "\n",
    "    def reset(self, initial_frame):\n",
    "        \"\"\"\n",
    "        Called at the start of an episode.\n",
    "        We take the very first raw frame, preprocess it,\n",
    "        and then duplicate it N times so the stacked state is well-defined.\n",
    "        This gives us a consistent starting state even though we only have one frame.\n",
    "        \"\"\"\n",
    "        # Preprocess the first frame to get it in the right format.\n",
    "        processed = preprocess_frame(initial_frame)\n",
    "\n",
    "        # Clear any old frames and fill the stack with copies of the first frame.\n",
    "        self.frames.clear()\n",
    "        for _ in range(self.num_frames):\n",
    "            self.frames.append(processed)\n",
    "\n",
    "        # Stack the frames along a new axis to create (num_frames, 84, 84) shape.\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "    def step(self, new_frame):\n",
    "        \"\"\"\n",
    "        Called every time we get a new raw frame from the environment.\n",
    "        We preprocess it and push it into the stack, automatically\n",
    "        dropping the oldest frame (thanks to deque's maxlen).\n",
    "        \"\"\"\n",
    "        # Preprocess the new frame.\n",
    "        processed = preprocess_frame(new_frame)\n",
    "        # Add it to the stack (oldest frame is automatically removed).\n",
    "        self.frames.append(processed)\n",
    "\n",
    "        # Stack all frames together to create the state tensor.\n",
    "        return np.stack(self.frames, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2b503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stacked state shape: (4, 84, 84)\n",
      "After step 1, state shape: (4, 84, 84)\n",
      "After step 2, state shape: (4, 84, 84)\n",
      "After step 3, state shape: (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Make a fresh environment reset so we have a clean starting frame.\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Create a frame stacker that holds the last 4 frames.\n",
    "frame_stack = FrameStack(num_frames=4)\n",
    "\n",
    "# Build the initial stacked state from the very first frame.\n",
    "state = frame_stack.reset(obs)\n",
    "print(\"Initial stacked state shape:\", state.shape)  # expected: (4, 84, 84)\n",
    "\n",
    "# Take a few random steps and update the frame stack.\n",
    "for t in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    state = frame_stack.step(obs)\n",
    "    print(f\"After step {t+1}, state shape:\", state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326ac7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple experience replay buffer.\n",
    "    Stores (state, action, reward, next_state, done) tuples.\n",
    "    \n",
    "    Experience replay is crucial for DQN - it breaks the correlation between\n",
    "    consecutive experiences and lets us learn from past experiences multiple times.\n",
    "\n",
    "    - state / next_state: stacked frames, shape (4, 84, 84)\n",
    "    - action: integer INDEX into VALID_ACTIONS (0..NUM_ACTIONS-1)\n",
    "    - reward: float (clipped to [-1, 0, +1])\n",
    "    - done: bool (True if the episode ended)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # 'capacity' is the max number of transitions we keep.\n",
    "        # Once we hit this limit, the oldest experiences get dropped automatically.\n",
    "        self.capacity = capacity\n",
    "        # Use a deque with maxlen so it automatically handles the capacity limit.\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Save a single experience into the buffer.\n",
    "        We store raw numpy arrays / values here (they get converted to tensors later).\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        This is what the DQN will train on - random sampling breaks correlation\n",
    "        between consecutive experiences which helps learning.\n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size experiences from the buffer.\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unzip the batch into separate arrays (one for each component).\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to numpy arrays for easier handling later.\n",
    "        # Stack states and next_states along a new batch dimension.\n",
    "        states      = np.stack(states, axis=0)        # (batch, 4, 84, 84)\n",
    "        next_states = np.stack(next_states, axis=0)   # (batch, 4, 84, 84)\n",
    "        actions     = np.array(actions, dtype=np.int64)\n",
    "        rewards     = np.array(rewards, dtype=np.float32)\n",
    "        dones       = np.array(dones, dtype=np.float32)  # will be 0.0 or 1.0\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Let len(buffer) tell us how many experiences we have stored.\n",
    "        This is handy for checking if we have enough to start training.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33701081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Deep Q-Network for Pong.\n",
    "    \n",
    "    This is the neural network that learns to predict Q-values (expected future rewards)\n",
    "    for each possible action in a given state. The architecture is standard for Atari DQN.\n",
    "\n",
    "    Input:  (batch, 4, 84, 84) stacked preprocessed frames\n",
    "    Output: (batch, num_actions) predicted Q-values (one per action)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # First block of convolutions extracts basic motion/edge features.\n",
    "        # Input: 4 frames (84x84 each), Output: 32 feature maps (20x20 each).\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "\n",
    "        # Second block extracts mid-level spatial features.\n",
    "        # Input: 32 feature maps (20x20), Output: 64 feature maps (9x9 each).\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "\n",
    "        # Third block captures more complex movement patterns.\n",
    "        # Input: 64 feature maps (9x9), Output: 64 feature maps (7x7 each).\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # The final size after these conv layers is 64 feature maps of size 7x7.\n",
    "        # So we have 64 * 7 * 7 = 3136 features to feed into the fully connected layers.\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "\n",
    "        # Final layer outputs one Q-value per action (3 actions for Pong: NOOP, UP, DOWN).\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution layer with ReLU activation (introduces non-linearity).\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # Second convolution layer with ReLU.\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Third convolution layer with ReLU.\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten the feature maps into a single vector before fully-connected layers.\n",
    "        # This converts (batch, 64, 7, 7) to (batch, 3136).\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Hidden fully-connected layer with ReLU activation.\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output layer: raw Q-values (no activation - these can be any real number).\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a7f950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device setup: use GPU if available, otherwise CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Hyperparameters for DQN.\n",
    "gamma = GAMMA          # discount factor for future rewards\n",
    "batch_size = BATCH_SIZE       # how many experiences to sample per training step\n",
    "learning_rate = LEARNING_RATE  # how fast we update the network weights\n",
    "\n",
    "# Create both the policy network and the target network.\n",
    "# - policy_net: the one we train on every step.\n",
    "# - target_net: a slowly updated copy used to compute stable target values.\n",
    "policy_net = DQN(num_actions=NUM_ACTIONS).to(device)\n",
    "target_net = DQN(num_actions=NUM_ACTIONS).to(device)\n",
    "\n",
    "# Copy the weights from policy_net -> target_net at the start.\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # target_net is not trained directly, only updated by copying.\n",
    "\n",
    "# Optimizer: Adam is a solid default choice for DQN.\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def optimize_model(replay_buffer, use_double_dqn=True):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step on the DQN using a batch of\n",
    "    experiences sampled from the replay buffer.\n",
    "\n",
    "    If use_double_dqn is True, we use a Double DQN-style target to\n",
    "    reduce maximisation bias:\n",
    "\n",
    "        a* = argmax_a Q_policy(s', a)\n",
    "        y  = r + gamma * Q_target(s', a*)\n",
    "\n",
    "    Otherwise we fall back to the standard DQN target:\n",
    "\n",
    "        y = r + gamma * max_a Q_target(s', a)\n",
    "    \"\"\"\n",
    "\n",
    "    # We need at least 'batch_size' experiences before we can train.\n",
    "    # Early in training, the buffer might not be full enough yet.\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None  # not enough data yet, skip the update\n",
    "\n",
    "    # ---- 1. Sample a batch of experiences from the replay buffer ----\n",
    "    # Randomly sample a batch of past experiences to learn from.\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and move them to the right device (CPU or GPU).\n",
    "    # This is necessary because PyTorch needs tensors, not numpy arrays.\n",
    "    states      = torch.from_numpy(states).to(device)          # (batch, 4, 84, 84)\n",
    "    next_states = torch.from_numpy(next_states).to(device)     # (batch, 4, 84, 84)\n",
    "    actions     = torch.from_numpy(actions).to(device)         # (batch,)\n",
    "    rewards     = torch.from_numpy(rewards).to(device)         # (batch,)\n",
    "    dones       = torch.from_numpy(dones).to(device)           # (batch,)\n",
    "\n",
    "    # ---- 2. Compute Q(s, a) for the actions actually taken ----\n",
    "    # Get Q-values for all possible actions in each state from our policy network.\n",
    "    # policy_net(states) gives Q-values for ALL actions in each state: shape (batch, num_actions).\n",
    "    q_values = policy_net(states)  # Q(s, ·)\n",
    "\n",
    "    # We only want the Q-values for the specific actions we actually took.\n",
    "    # gather() selects the Q-value for each action we took in each state.\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Q(s, a_taken)\n",
    "\n",
    "    # ---- 3. Compute target values ----\n",
    "    # We use torch.no_grad() because we don't want to backpropagate through the target network.\n",
    "    # The target network is just for computing stable Q-value estimates.\n",
    "    with torch.no_grad():\n",
    "        if use_double_dqn:\n",
    "            # -------- Double DQN target (reduces overestimation bias) --------\n",
    "            # Step 1: Use the *policy* network to pick the best next action.\n",
    "            # This is the key difference from standard DQN - we use policy net to select.\n",
    "            next_q_policy = policy_net(next_states)                 # Q_policy(s', ·)\n",
    "            best_next_actions = next_q_policy.argmax(dim=1)         # a* for each sample\n",
    "\n",
    "            # Step 2: Use the *target* network to evaluate those actions.\n",
    "            # This gives us a more stable estimate of the Q-value.\n",
    "            next_q_target = target_net(next_states)                 # Q_target(s', ·)\n",
    "            max_next_q_values = next_q_target.gather(\n",
    "                1, best_next_actions.unsqueeze(1)\n",
    "            ).squeeze(1)                                           # Q_target(s', a*)\n",
    "        else:\n",
    "            # -------- Standard DQN target --------\n",
    "            # Just use the target network to find the max Q-value in the next state.\n",
    "            next_q_values = target_net(next_states)                 # Q_target(s', ·)\n",
    "            max_next_q_values, _ = next_q_values.max(dim=1)        # max_a Q_target(s', a)\n",
    "\n",
    "        # Compute the target Q-value using the Bellman equation.\n",
    "        # If done == 1, we don't bootstrap from the next state (no future reward).\n",
    "        # This is the target we want our Q(s,a) to match.\n",
    "        target_values = rewards + gamma * max_next_q_values * (1.0 - dones)\n",
    "\n",
    "    # ---- 4. Compute the loss between current Q(s, a) and target values ----\n",
    "    # The loss measures how far off our Q-value predictions are from the targets.\n",
    "    # We use mean squared error (MSE) which is standard for DQN.\n",
    "    loss = F.mse_loss(state_action_values, target_values)\n",
    "\n",
    "    # ---- 5. Backpropagation: update the policy network weights ----\n",
    "    optimizer.zero_grad()  # clear old gradients from previous step\n",
    "    loss.backward()        # compute new gradients through the network\n",
    "    \n",
    "    # Clip gradients to prevent them from getting too large (helps training stability).\n",
    "    # This prevents the network from making huge updates that could break learning.\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=10.0)\n",
    "    \n",
    "    optimizer.step()       # update the weights using the gradients\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0794069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will track how many environment steps we've taken so far.\n",
    "# We'll use it to slowly decay epsilon over time.\n",
    "steps_done = 0\n",
    "\n",
    "def get_epsilon(step):\n",
    "    \"\"\"\n",
    "    Linearly decay epsilon from epsilon_start down to epsilon_end\n",
    "    over 'epsilon_decay' steps.\n",
    "\n",
    "    Epsilon controls exploration: high epsilon = more random actions (explore),\n",
    "    low epsilon = more greedy actions (exploit what we've learned).\n",
    "    \n",
    "    After that many steps, epsilon stays at epsilon_end.\n",
    "    \"\"\"\n",
    "    # If we've passed the decay period, just return the minimum epsilon.\n",
    "    if step >= EPSILON_DECAY:\n",
    "        return EPSILON_END\n",
    "    \n",
    "    # Linear interpolation between start and end based on how many steps we've taken.\n",
    "    # This gives us a smooth transition from exploration to exploitation.\n",
    "    fraction = step / EPSILON_DECAY\n",
    "    return EPSILON_START + fraction * (EPSILON_END - EPSILON_START)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Choose an action using an epsilon-greedy policy over the\n",
    "    REDUCED action set (VALID_ACTIONS).\n",
    "\n",
    "    Epsilon-greedy means: with probability epsilon, explore (random action),\n",
    "    otherwise exploit (best action according to Q-network).\n",
    "\n",
    "    - The DQN outputs Q-values for action indices: 0, 1, 2\n",
    "    - We map those indices to real Atari actions via VALID_ACTIONS.\n",
    "\n",
    "    Returns:\n",
    "        action_index: integer in [0, NUM_ACTIONS-1]\n",
    "        env_action:   actual action ID to pass to env.step(...)\n",
    "        epsilon:      current exploration rate (for logging)\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "\n",
    "    # Compute current epsilon based on how many action selections we've made.\n",
    "    # Epsilon decays over time, so we explore less as we learn more.\n",
    "    epsilon = get_epsilon(steps_done)\n",
    "    steps_done += 1  # Increment the step counter for epsilon decay.\n",
    "\n",
    "    # Decide: explore (random) or exploit (greedy)?\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: pick a random INDEX into VALID_ACTIONS.\n",
    "        # This helps us discover new strategies and avoid getting stuck.\n",
    "        action_index = np.random.randint(NUM_ACTIONS)\n",
    "        env_action = VALID_ACTIONS[action_index]\n",
    "        return action_index, env_action, epsilon\n",
    "\n",
    "    # Exploit: choose best action index according to the current Q-network.\n",
    "    # We need to turn state into a torch tensor with shape (1, 4, 84, 84).\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)  # add batch dimension\n",
    "\n",
    "    # Use the network to predict Q-values, then pick the action with highest Q-value.\n",
    "    with torch.no_grad():  # Don't compute gradients - we're just selecting an action.\n",
    "        q_values = policy_net(state_tensor)  # shape: (1, NUM_ACTIONS)\n",
    "        action_index = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    # Convert the action index to the actual Atari action code.\n",
    "    env_action = VALID_ACTIONS[action_index]\n",
    "    return action_index, env_action, epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13483b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2025-12-01 18:26:23.703083\n",
      "Starting training...\n",
      "\n",
      "Episode    1/1500 | Reward: -21.0 | Epsilon: 0.997 | Buffer: 764 | Steps: 764\n",
      "Episode    2/1500 | Reward: -21.0 | Epsilon: 0.993 | Buffer: 1831 | Steps: 1831\n",
      "Episode    3/1500 | Reward: -21.0 | Epsilon: 0.990 | Buffer: 2715 | Steps: 2715\n",
      "Episode    4/1500 | Reward: -21.0 | Epsilon: 0.987 | Buffer: 3540 | Steps: 3540\n",
      "Episode    5/1500 | Reward: -21.0 | Epsilon: 0.985 | Buffer: 4304 | Steps: 4304\n",
      "Episode    6/1500 | Reward: -21.0 | Epsilon: 0.980 | Buffer: 5555 | Steps: 5555\n",
      "Episode    7/1500 | Reward: -20.0 | Epsilon: 0.977 | Buffer: 6474 | Steps: 6474\n",
      "Episode    8/1500 | Reward: -18.0 | Epsilon: 0.972 | Buffer: 7768 | Steps: 7768\n",
      "Episode    9/1500 | Reward: -21.0 | Epsilon: 0.969 | Buffer: 8532 | Steps: 8532\n",
      "Episode   10/1500 | Reward: -21.0 | Epsilon: 0.967 | Buffer: 9296 | Steps: 9296\n",
      "Episode   11/1500 | Reward: -21.0 | Epsilon: 0.964 | Buffer: 10060 | Steps: 10060\n",
      "Episode   12/1500 | Reward: -20.0 | Epsilon: 0.960 | Buffer: 11041 | Steps: 11041\n",
      "Episode   13/1500 | Reward: -21.0 | Epsilon: 0.957 | Buffer: 11985 | Steps: 11985\n",
      "Episode   14/1500 | Reward: -21.0 | Epsilon: 0.954 | Buffer: 12870 | Steps: 12870\n",
      "Episode   15/1500 | Reward: -21.0 | Epsilon: 0.950 | Buffer: 13875 | Steps: 13875\n",
      "Episode   16/1500 | Reward: -20.0 | Epsilon: 0.945 | Buffer: 15159 | Steps: 15159\n",
      "Episode   17/1500 | Reward: -21.0 | Epsilon: 0.942 | Buffer: 16225 | Steps: 16225\n",
      "Episode   18/1500 | Reward: -20.0 | Epsilon: 0.938 | Buffer: 17205 | Steps: 17205\n",
      "Episode   19/1500 | Reward: -20.0 | Epsilon: 0.934 | Buffer: 18306 | Steps: 18306\n",
      "Episode   20/1500 | Reward: -21.0 | Epsilon: 0.931 | Buffer: 19070 | Steps: 19070\n",
      "Episode   21/1500 | Reward: -18.0 | Epsilon: 0.926 | Buffer: 20420 | Steps: 20420\n",
      "Episode   22/1500 | Reward: -21.0 | Epsilon: 0.923 | Buffer: 21426 | Steps: 21426\n",
      "Episode   23/1500 | Reward: -21.0 | Epsilon: 0.920 | Buffer: 22250 | Steps: 22250\n",
      "Episode   24/1500 | Reward: -21.0 | Epsilon: 0.917 | Buffer: 23014 | Steps: 23014\n",
      "Episode   25/1500 | Reward: -21.0 | Epsilon: 0.914 | Buffer: 23840 | Steps: 23840\n",
      "Episode   26/1500 | Reward: -21.0 | Epsilon: 0.911 | Buffer: 24725 | Steps: 24725\n",
      "Episode   27/1500 | Reward: -21.0 | Epsilon: 0.908 | Buffer: 25489 | Steps: 25489\n",
      "Episode   28/1500 | Reward: -21.0 | Epsilon: 0.905 | Buffer: 26438 | Steps: 26438\n",
      "Episode   29/1500 | Reward: -21.0 | Epsilon: 0.901 | Buffer: 27384 | Steps: 27384\n",
      "Episode   30/1500 | Reward: -21.0 | Epsilon: 0.898 | Buffer: 28392 | Steps: 28392\n",
      "Episode   31/1500 | Reward: -21.0 | Epsilon: 0.894 | Buffer: 29400 | Steps: 29400\n",
      "Episode   32/1500 | Reward: -21.0 | Epsilon: 0.891 | Buffer: 30284 | Steps: 30284\n",
      "Episode   33/1500 | Reward: -21.0 | Epsilon: 0.888 | Buffer: 31169 | Steps: 31169\n",
      "Episode   34/1500 | Reward: -21.0 | Epsilon: 0.884 | Buffer: 32177 | Steps: 32177\n",
      "Episode   35/1500 | Reward: -20.0 | Epsilon: 0.880 | Buffer: 33221 | Steps: 33221\n",
      "Episode   36/1500 | Reward: -21.0 | Epsilon: 0.876 | Buffer: 34347 | Steps: 34347\n",
      "Episode   37/1500 | Reward: -21.0 | Epsilon: 0.874 | Buffer: 35111 | Steps: 35111\n",
      "Episode   38/1500 | Reward: -19.0 | Epsilon: 0.869 | Buffer: 36307 | Steps: 36307\n",
      "Episode   39/1500 | Reward: -21.0 | Epsilon: 0.866 | Buffer: 37195 | Steps: 37195\n",
      "Episode   40/1500 | Reward: -21.0 | Epsilon: 0.862 | Buffer: 38199 | Steps: 38199\n",
      "Episode   41/1500 | Reward: -21.0 | Epsilon: 0.858 | Buffer: 39326 | Steps: 39326\n",
      "Episode   42/1500 | Reward: -20.0 | Epsilon: 0.854 | Buffer: 40427 | Steps: 40427\n",
      "Episode   43/1500 | Reward: -21.0 | Epsilon: 0.851 | Buffer: 41251 | Steps: 41251\n",
      "Episode   44/1500 | Reward: -20.0 | Epsilon: 0.847 | Buffer: 42478 | Steps: 42478\n",
      "Episode   45/1500 | Reward: -21.0 | Epsilon: 0.844 | Buffer: 43423 | Steps: 43423\n",
      "Episode   46/1500 | Reward: -20.0 | Epsilon: 0.839 | Buffer: 44585 | Steps: 44585\n",
      "Episode   47/1500 | Reward: -21.0 | Epsilon: 0.836 | Buffer: 45595 | Steps: 45595\n",
      "Episode   48/1500 | Reward: -20.0 | Epsilon: 0.832 | Buffer: 46699 | Steps: 46699\n",
      "Episode   49/1500 | Reward: -21.0 | Epsilon: 0.829 | Buffer: 47583 | Steps: 47583\n",
      "Episode   50/1500 | Reward: -19.0 | Epsilon: 0.825 | Buffer: 48660 | Steps: 48660\n",
      "Episode   51/1500 | Reward: -20.0 | Epsilon: 0.821 | Buffer: 49641 | Steps: 49641\n",
      "Episode   52/1500 | Reward: -21.0 | Epsilon: 0.818 | Buffer: 50588 | Steps: 50588\n",
      "Episode   53/1500 | Reward: -20.0 | Epsilon: 0.814 | Buffer: 51568 | Steps: 51568\n",
      "Episode   54/1500 | Reward: -20.0 | Epsilon: 0.811 | Buffer: 52612 | Steps: 52612\n",
      "Episode   55/1500 | Reward: -20.0 | Epsilon: 0.807 | Buffer: 53532 | Steps: 53532\n",
      "Episode   56/1500 | Reward: -21.0 | Epsilon: 0.804 | Buffer: 54418 | Steps: 54418\n",
      "Episode   57/1500 | Reward: -19.0 | Epsilon: 0.800 | Buffer: 55672 | Steps: 55672\n",
      "Episode   58/1500 | Reward: -21.0 | Epsilon: 0.797 | Buffer: 56496 | Steps: 56496\n",
      "Episode   59/1500 | Reward: -20.0 | Epsilon: 0.793 | Buffer: 57538 | Steps: 57538\n",
      "Episode   60/1500 | Reward: -21.0 | Epsilon: 0.790 | Buffer: 58424 | Steps: 58424\n",
      "Episode   61/1500 | Reward: -19.0 | Epsilon: 0.785 | Buffer: 59744 | Steps: 59744\n",
      "Episode   62/1500 | Reward: -21.0 | Epsilon: 0.782 | Buffer: 60570 | Steps: 60570\n",
      "Episode   63/1500 | Reward: -21.0 | Epsilon: 0.779 | Buffer: 61394 | Steps: 61394\n",
      "Episode   64/1500 | Reward: -21.0 | Epsilon: 0.776 | Buffer: 62278 | Steps: 62278\n",
      "Episode   65/1500 | Reward: -21.0 | Epsilon: 0.772 | Buffer: 63227 | Steps: 63227\n",
      "Episode   66/1500 | Reward: -21.0 | Epsilon: 0.769 | Buffer: 64113 | Steps: 64113\n",
      "Episode   67/1500 | Reward: -20.0 | Epsilon: 0.766 | Buffer: 65034 | Steps: 65034\n",
      "Episode   68/1500 | Reward: -18.0 | Epsilon: 0.761 | Buffer: 66446 | Steps: 66446\n",
      "Episode   69/1500 | Reward: -20.0 | Epsilon: 0.757 | Buffer: 67605 | Steps: 67605\n",
      "Episode   70/1500 | Reward: -20.0 | Epsilon: 0.752 | Buffer: 68768 | Steps: 68768\n",
      "Episode   71/1500 | Reward: -21.0 | Epsilon: 0.749 | Buffer: 69776 | Steps: 69776\n",
      "Episode   72/1500 | Reward: -21.0 | Epsilon: 0.746 | Buffer: 70664 | Steps: 70664\n",
      "Episode   73/1500 | Reward: -20.0 | Epsilon: 0.742 | Buffer: 71648 | Steps: 71648\n",
      "Episode   74/1500 | Reward: -21.0 | Epsilon: 0.738 | Buffer: 72653 | Steps: 72653\n",
      "Episode   75/1500 | Reward: -21.0 | Epsilon: 0.735 | Buffer: 73539 | Steps: 73539\n",
      "Episode   76/1500 | Reward: -21.0 | Epsilon: 0.732 | Buffer: 74423 | Steps: 74423\n",
      "Episode   77/1500 | Reward: -21.0 | Epsilon: 0.729 | Buffer: 75368 | Steps: 75368\n",
      "Episode   78/1500 | Reward: -21.0 | Epsilon: 0.725 | Buffer: 76377 | Steps: 76377\n",
      "Episode   79/1500 | Reward: -20.0 | Epsilon: 0.721 | Buffer: 77481 | Steps: 77481\n",
      "Episode   80/1500 | Reward: -21.0 | Epsilon: 0.718 | Buffer: 78369 | Steps: 78369\n",
      "Episode   81/1500 | Reward: -21.0 | Epsilon: 0.715 | Buffer: 79253 | Steps: 79253\n",
      "Episode   82/1500 | Reward: -20.0 | Epsilon: 0.711 | Buffer: 80414 | Steps: 80414\n",
      "Episode   83/1500 | Reward: -21.0 | Epsilon: 0.707 | Buffer: 81298 | Steps: 81298\n",
      "Episode   84/1500 | Reward: -18.0 | Epsilon: 0.703 | Buffer: 82589 | Steps: 82589\n",
      "Episode   85/1500 | Reward: -21.0 | Epsilon: 0.699 | Buffer: 83718 | Steps: 83718\n",
      "Episode   86/1500 | Reward: -21.0 | Epsilon: 0.695 | Buffer: 84602 | Steps: 84602\n",
      "Episode   87/1500 | Reward: -21.0 | Epsilon: 0.692 | Buffer: 85550 | Steps: 85550\n",
      "Episode   88/1500 | Reward: -21.0 | Epsilon: 0.688 | Buffer: 86556 | Steps: 86556\n",
      "Episode   89/1500 | Reward: -21.0 | Epsilon: 0.685 | Buffer: 87442 | Steps: 87442\n",
      "Episode   90/1500 | Reward: -21.0 | Epsilon: 0.682 | Buffer: 88328 | Steps: 88328\n",
      "Episode   91/1500 | Reward: -21.0 | Epsilon: 0.679 | Buffer: 89154 | Steps: 89154\n",
      "Episode   92/1500 | Reward: -20.0 | Epsilon: 0.676 | Buffer: 90073 | Steps: 90073\n",
      "Episode   93/1500 | Reward: -21.0 | Epsilon: 0.673 | Buffer: 90899 | Steps: 90899\n",
      "Episode   94/1500 | Reward: -20.0 | Epsilon: 0.669 | Buffer: 91940 | Steps: 91940\n",
      "Episode   95/1500 | Reward: -21.0 | Epsilon: 0.666 | Buffer: 92886 | Steps: 92886\n",
      "Episode   96/1500 | Reward: -21.0 | Epsilon: 0.663 | Buffer: 93712 | Steps: 93712\n",
      "Episode   97/1500 | Reward: -21.0 | Epsilon: 0.660 | Buffer: 94536 | Steps: 94536\n",
      "Episode   98/1500 | Reward: -21.0 | Epsilon: 0.657 | Buffer: 95362 | Steps: 95362\n",
      "Episode   99/1500 | Reward: -21.0 | Epsilon: 0.653 | Buffer: 96310 | Steps: 96310\n",
      "Episode  100/1500 | Reward: -21.0 | Epsilon: 0.650 | Buffer: 97197 | Steps: 97197\n",
      "Episode  101/1500 | Reward: -21.0 | Epsilon: 0.647 | Buffer: 98021 | Steps: 98021\n",
      "Episode  102/1500 | Reward: -21.0 | Epsilon: 0.644 | Buffer: 98847 | Steps: 98847\n",
      "Episode  103/1500 | Reward: -21.0 | Epsilon: 0.641 | Buffer: 99671 | Steps: 99671\n",
      "Episode  104/1500 | Reward: -21.0 | Epsilon: 0.638 | Buffer: 100000 | Steps: 100677\n",
      "Episode  105/1500 | Reward: -21.0 | Epsilon: 0.635 | Buffer: 100000 | Steps: 101501\n",
      "Episode  106/1500 | Reward: -21.0 | Epsilon: 0.631 | Buffer: 100000 | Steps: 102450\n",
      "Episode  107/1500 | Reward: -21.0 | Epsilon: 0.628 | Buffer: 100000 | Steps: 103334\n",
      "Episode  108/1500 | Reward: -20.0 | Epsilon: 0.624 | Buffer: 100000 | Steps: 104313\n",
      "Episode  109/1500 | Reward: -21.0 | Epsilon: 0.621 | Buffer: 100000 | Steps: 105139\n",
      "Episode  110/1500 | Reward: -20.0 | Epsilon: 0.618 | Buffer: 100000 | Steps: 106121\n",
      "Episode  111/1500 | Reward: -21.0 | Epsilon: 0.615 | Buffer: 100000 | Steps: 106946\n",
      "Episode  112/1500 | Reward: -21.0 | Epsilon: 0.611 | Buffer: 100000 | Steps: 108013\n",
      "Episode  113/1500 | Reward: -21.0 | Epsilon: 0.608 | Buffer: 100000 | Steps: 108839\n",
      "Episode  114/1500 | Reward: -20.0 | Epsilon: 0.605 | Buffer: 100000 | Steps: 109760\n",
      "Episode  115/1500 | Reward: -21.0 | Epsilon: 0.602 | Buffer: 100000 | Steps: 110584\n",
      "Episode  116/1500 | Reward: -21.0 | Epsilon: 0.599 | Buffer: 100000 | Steps: 111409\n",
      "Episode  117/1500 | Reward: -21.0 | Epsilon: 0.596 | Buffer: 100000 | Steps: 112234\n",
      "Episode  118/1500 | Reward: -21.0 | Epsilon: 0.593 | Buffer: 100000 | Steps: 113180\n",
      "Episode  119/1500 | Reward: -21.0 | Epsilon: 0.590 | Buffer: 100000 | Steps: 114004\n",
      "Episode  120/1500 | Reward: -21.0 | Epsilon: 0.587 | Buffer: 100000 | Steps: 114828\n",
      "Episode  121/1500 | Reward: -21.0 | Epsilon: 0.584 | Buffer: 100000 | Steps: 115654\n",
      "Episode  122/1500 | Reward: -21.0 | Epsilon: 0.580 | Buffer: 100000 | Steps: 116781\n",
      "Episode  123/1500 | Reward: -21.0 | Epsilon: 0.577 | Buffer: 100000 | Steps: 117605\n",
      "Episode  124/1500 | Reward: -21.0 | Epsilon: 0.574 | Buffer: 100000 | Steps: 118430\n",
      "Episode  125/1500 | Reward: -21.0 | Epsilon: 0.570 | Buffer: 100000 | Steps: 119435\n",
      "Episode  126/1500 | Reward: -21.0 | Epsilon: 0.567 | Buffer: 100000 | Steps: 120380\n",
      "Episode  127/1500 | Reward: -21.0 | Epsilon: 0.563 | Buffer: 100000 | Steps: 121328\n",
      "Episode  128/1500 | Reward: -21.0 | Epsilon: 0.560 | Buffer: 100000 | Steps: 122153\n",
      "Episode  129/1500 | Reward: -21.0 | Epsilon: 0.557 | Buffer: 100000 | Steps: 122979\n",
      "Episode  130/1500 | Reward: -21.0 | Epsilon: 0.554 | Buffer: 100000 | Steps: 123803\n",
      "Episode  131/1500 | Reward: -21.0 | Epsilon: 0.551 | Buffer: 100000 | Steps: 124687\n",
      "Episode  132/1500 | Reward: -21.0 | Epsilon: 0.548 | Buffer: 100000 | Steps: 125511\n",
      "Episode  133/1500 | Reward: -21.0 | Epsilon: 0.545 | Buffer: 100000 | Steps: 126397\n",
      "Episode  134/1500 | Reward: -21.0 | Epsilon: 0.542 | Buffer: 100000 | Steps: 127222\n",
      "Episode  135/1500 | Reward: -20.0 | Epsilon: 0.538 | Buffer: 100000 | Steps: 128383\n",
      "Episode  136/1500 | Reward: -21.0 | Epsilon: 0.534 | Buffer: 100000 | Steps: 129389\n",
      "Episode  137/1500 | Reward: -21.0 | Epsilon: 0.531 | Buffer: 100000 | Steps: 130335\n",
      "Episode  138/1500 | Reward: -21.0 | Epsilon: 0.528 | Buffer: 100000 | Steps: 131219\n",
      "Episode  139/1500 | Reward: -21.0 | Epsilon: 0.524 | Buffer: 100000 | Steps: 132163\n",
      "Episode  140/1500 | Reward: -21.0 | Epsilon: 0.521 | Buffer: 100000 | Steps: 133109\n",
      "Episode  141/1500 | Reward: -21.0 | Epsilon: 0.518 | Buffer: 100000 | Steps: 133995\n",
      "Episode  142/1500 | Reward: -21.0 | Epsilon: 0.514 | Buffer: 100000 | Steps: 134883\n",
      "Episode  143/1500 | Reward: -21.0 | Epsilon: 0.511 | Buffer: 100000 | Steps: 135767\n",
      "Episode  144/1500 | Reward: -21.0 | Epsilon: 0.508 | Buffer: 100000 | Steps: 136591\n",
      "Episode  145/1500 | Reward: -21.0 | Epsilon: 0.504 | Buffer: 100000 | Steps: 137661\n",
      "Episode  146/1500 | Reward: -21.0 | Epsilon: 0.501 | Buffer: 100000 | Steps: 138667\n",
      "Episode  147/1500 | Reward: -21.0 | Epsilon: 0.497 | Buffer: 100000 | Steps: 139611\n",
      "Episode  148/1500 | Reward: -21.0 | Epsilon: 0.494 | Buffer: 100000 | Steps: 140495\n",
      "Episode  149/1500 | Reward: -21.0 | Epsilon: 0.491 | Buffer: 100000 | Steps: 141381\n",
      "Episode  150/1500 | Reward: -21.0 | Epsilon: 0.488 | Buffer: 100000 | Steps: 142207\n",
      "Episode  151/1500 | Reward: -21.0 | Epsilon: 0.485 | Buffer: 100000 | Steps: 143033\n",
      "Episode  152/1500 | Reward: -21.0 | Epsilon: 0.482 | Buffer: 100000 | Steps: 143920\n",
      "Episode  153/1500 | Reward: -21.0 | Epsilon: 0.478 | Buffer: 100000 | Steps: 144928\n",
      "Episode  154/1500 | Reward: -21.0 | Epsilon: 0.475 | Buffer: 100000 | Steps: 145753\n",
      "Episode  155/1500 | Reward: -21.0 | Epsilon: 0.472 | Buffer: 100000 | Steps: 146638\n",
      "Episode  156/1500 | Reward: -21.0 | Epsilon: 0.469 | Buffer: 100000 | Steps: 147522\n",
      "Episode  157/1500 | Reward: -20.0 | Epsilon: 0.464 | Buffer: 100000 | Steps: 148928\n",
      "Episode  158/1500 | Reward: -21.0 | Epsilon: 0.461 | Buffer: 100000 | Steps: 149754\n",
      "Episode  159/1500 | Reward: -20.0 | Epsilon: 0.457 | Buffer: 100000 | Steps: 150859\n",
      "Episode  160/1500 | Reward: -21.0 | Epsilon: 0.450 | Buffer: 100000 | Steps: 152719\n",
      "Episode  161/1500 | Reward: -19.0 | Epsilon: 0.445 | Buffer: 100000 | Steps: 154222\n",
      "Episode  162/1500 | Reward: -20.0 | Epsilon: 0.441 | Buffer: 100000 | Steps: 155383\n",
      "Episode  163/1500 | Reward: -21.0 | Epsilon: 0.437 | Buffer: 100000 | Steps: 156511\n",
      "Episode  164/1500 | Reward: -17.0 | Epsilon: 0.432 | Buffer: 100000 | Steps: 157776\n",
      "Episode  165/1500 | Reward: -21.0 | Epsilon: 0.428 | Buffer: 100000 | Steps: 159026\n",
      "Episode  166/1500 | Reward: -20.0 | Epsilon: 0.423 | Buffer: 100000 | Steps: 160252\n",
      "Episode  167/1500 | Reward: -21.0 | Epsilon: 0.420 | Buffer: 100000 | Steps: 161198\n",
      "Episode  168/1500 | Reward: -20.0 | Epsilon: 0.415 | Buffer: 100000 | Steps: 162480\n",
      "Episode  169/1500 | Reward: -20.0 | Epsilon: 0.409 | Buffer: 100000 | Steps: 164128\n",
      "Episode  170/1500 | Reward: -20.0 | Epsilon: 0.404 | Buffer: 100000 | Steps: 165589\n",
      "Episode  171/1500 | Reward: -20.0 | Epsilon: 0.400 | Buffer: 100000 | Steps: 166696\n",
      "Episode  172/1500 | Reward: -21.0 | Epsilon: 0.396 | Buffer: 100000 | Steps: 167640\n",
      "Episode  173/1500 | Reward: -21.0 | Epsilon: 0.393 | Buffer: 100000 | Steps: 168710\n",
      "Episode  174/1500 | Reward: -16.0 | Epsilon: 0.386 | Buffer: 100000 | Steps: 170678\n",
      "Episode  175/1500 | Reward: -21.0 | Epsilon: 0.380 | Buffer: 100000 | Steps: 172103\n",
      "Episode  176/1500 | Reward: -21.0 | Epsilon: 0.376 | Buffer: 100000 | Steps: 173416\n",
      "Episode  177/1500 | Reward: -21.0 | Epsilon: 0.371 | Buffer: 100000 | Steps: 174787\n",
      "Episode  178/1500 | Reward: -21.0 | Epsilon: 0.365 | Buffer: 100000 | Steps: 176396\n",
      "Episode  179/1500 | Reward: -19.0 | Epsilon: 0.359 | Buffer: 100000 | Steps: 178135\n",
      "Episode  180/1500 | Reward: -20.0 | Epsilon: 0.355 | Buffer: 100000 | Steps: 179297\n",
      "Episode  181/1500 | Reward: -21.0 | Epsilon: 0.349 | Buffer: 100000 | Steps: 180795\n",
      "Episode  182/1500 | Reward: -21.0 | Epsilon: 0.345 | Buffer: 100000 | Steps: 182041\n",
      "Episode  183/1500 | Reward: -20.0 | Epsilon: 0.340 | Buffer: 100000 | Steps: 183203\n",
      "Episode  184/1500 | Reward: -21.0 | Epsilon: 0.335 | Buffer: 100000 | Steps: 184633\n",
      "Episode  185/1500 | Reward: -18.0 | Epsilon: 0.329 | Buffer: 100000 | Steps: 186287\n",
      "Episode  186/1500 | Reward: -19.0 | Epsilon: 0.323 | Buffer: 100000 | Steps: 188147\n",
      "Episode  187/1500 | Reward: -19.0 | Epsilon: 0.316 | Buffer: 100000 | Steps: 190075\n",
      "Episode  188/1500 | Reward: -19.0 | Epsilon: 0.311 | Buffer: 100000 | Steps: 191514\n",
      "Episode  189/1500 | Reward: -19.0 | Epsilon: 0.304 | Buffer: 100000 | Steps: 193316\n",
      "Episode  190/1500 | Reward: -18.0 | Epsilon: 0.298 | Buffer: 100000 | Steps: 195029\n",
      "Episode  191/1500 | Reward: -20.0 | Epsilon: 0.292 | Buffer: 100000 | Steps: 196614\n",
      "Episode  192/1500 | Reward: -19.0 | Epsilon: 0.285 | Buffer: 100000 | Steps: 198662\n",
      "Episode  193/1500 | Reward: -16.0 | Epsilon: 0.278 | Buffer: 100000 | Steps: 200506\n",
      "Episode  194/1500 | Reward: -20.0 | Epsilon: 0.271 | Buffer: 100000 | Steps: 202514\n",
      "Episode  195/1500 | Reward: -17.0 | Epsilon: 0.265 | Buffer: 100000 | Steps: 204140\n",
      "Episode  196/1500 | Reward: -19.0 | Epsilon: 0.258 | Buffer: 100000 | Steps: 206181\n",
      "Episode  197/1500 | Reward: -17.0 | Epsilon: 0.250 | Buffer: 100000 | Steps: 208294\n",
      "Episode  198/1500 | Reward: -18.0 | Epsilon: 0.244 | Buffer: 100000 | Steps: 210130\n",
      "Episode  199/1500 | Reward: -14.0 | Epsilon: 0.235 | Buffer: 100000 | Steps: 212588\n",
      "Episode  200/1500 | Reward: -17.0 | Epsilon: 0.228 | Buffer: 100000 | Steps: 214517\n",
      "Episode  201/1500 | Reward: -18.0 | Epsilon: 0.222 | Buffer: 100000 | Steps: 216170\n",
      "Episode  202/1500 | Reward: -15.0 | Epsilon: 0.214 | Buffer: 100000 | Steps: 218235\n",
      "Episode  203/1500 | Reward: -19.0 | Epsilon: 0.207 | Buffer: 100000 | Steps: 220337\n",
      "Episode  204/1500 | Reward: -19.0 | Epsilon: 0.200 | Buffer: 100000 | Steps: 222138\n",
      "Episode  205/1500 | Reward: -21.0 | Epsilon: 0.193 | Buffer: 100000 | Steps: 224177\n",
      "Episode  206/1500 | Reward: -14.0 | Epsilon: 0.184 | Buffer: 100000 | Steps: 226759\n",
      "Episode  207/1500 | Reward: -18.0 | Epsilon: 0.176 | Buffer: 100000 | Steps: 228774\n",
      "Episode  208/1500 | Reward: -20.0 | Epsilon: 0.169 | Buffer: 100000 | Steps: 230903\n",
      "Episode  209/1500 | Reward: -15.0 | Epsilon: 0.162 | Buffer: 100000 | Steps: 232844\n",
      "Episode  210/1500 | Reward: -18.0 | Epsilon: 0.153 | Buffer: 100000 | Steps: 235160\n",
      "Episode  211/1500 | Reward: -18.0 | Epsilon: 0.146 | Buffer: 100000 | Steps: 237175\n",
      "Episode  212/1500 | Reward: -16.0 | Epsilon: 0.137 | Buffer: 100000 | Steps: 239622\n",
      "Episode  213/1500 | Reward: -20.0 | Epsilon: 0.130 | Buffer: 100000 | Steps: 241569\n",
      "Episode  214/1500 | Reward: -16.0 | Epsilon: 0.123 | Buffer: 100000 | Steps: 243653\n",
      "Episode  215/1500 | Reward: -17.0 | Epsilon: 0.113 | Buffer: 100000 | Steps: 246366\n",
      "Episode  216/1500 | Reward: -17.0 | Epsilon: 0.104 | Buffer: 100000 | Steps: 248897\n",
      "Episode  217/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 251595\n",
      "Episode  218/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 254125\n",
      "Episode  219/1500 | Reward: -15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 257463\n",
      "Episode  220/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 260463\n",
      "Episode  221/1500 | Reward: -15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 264154\n",
      "Episode  222/1500 | Reward: -15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 267428\n",
      "Episode  223/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 271918\n",
      "Episode  224/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 274621\n",
      "Episode  225/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 277642\n",
      "Episode  226/1500 | Reward: -10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 281453\n",
      "Episode  227/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 284653\n",
      "Episode  228/1500 | Reward: -19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 287292\n",
      "Episode  229/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 290554\n",
      "Episode  230/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 293476\n",
      "Episode  231/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 297412\n",
      "Episode  232/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 300734\n",
      "Episode  233/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 304652\n",
      "Episode  234/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 308606\n",
      "Episode  235/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 312204\n",
      "Episode  236/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 315301\n",
      "Episode  237/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 318415\n",
      "Episode  238/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 321414\n",
      "Episode  239/1500 | Reward: -10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 324676\n",
      "Episode  240/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 327675\n",
      "Episode  241/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 331230\n",
      "Episode  242/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 335919\n",
      "Episode  243/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 339452\n",
      "Episode  244/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 342743\n",
      "Episode  245/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 345309\n",
      "Episode  246/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 349409\n",
      "Episode  247/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 352891\n",
      "Episode  248/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 356832\n",
      "Episode  249/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 359397\n",
      "Episode  250/1500 | Reward:  -9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 363236\n",
      "Episode  251/1500 | Reward: -11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 367315\n",
      "Episode  252/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 372284\n",
      "Episode  253/1500 | Reward: -16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 377516\n",
      "Episode  254/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 381187\n",
      "Episode  255/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 385667\n",
      "Episode  256/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 390880\n",
      "Episode  257/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 398639\n",
      "Episode  258/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 404862\n",
      "Episode  259/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 409259\n",
      "Episode  260/1500 | Reward:  -9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 414422\n",
      "Episode  261/1500 | Reward:  -4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 420066\n",
      "Episode  262/1500 | Reward: -18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 424613\n",
      "Episode  263/1500 | Reward:  -9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 428695\n",
      "Episode  264/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 434321\n",
      "Episode  265/1500 | Reward:  -7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 440297\n",
      "Episode  266/1500 | Reward: -11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 446721\n",
      "Episode  267/1500 | Reward:  -4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 454010\n",
      "Episode  268/1500 | Reward: -12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 458286\n",
      "Episode  269/1500 | Reward: -13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 461681\n",
      "Episode  270/1500 | Reward: -17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 466440\n",
      "Episode  271/1500 | Reward:  -4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 473887\n",
      "Episode  272/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 481081\n",
      "Episode  273/1500 | Reward:  -7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 486077\n",
      "Episode  274/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 491700\n",
      "Episode  275/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 499378\n",
      "Episode  276/1500 | Reward: -11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 503751\n",
      "Episode  277/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 511605\n",
      "Episode  278/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 518254\n",
      "Episode  279/1500 | Reward: -11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 523411\n",
      "Episode  280/1500 | Reward:  -7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 530885\n",
      "Episode  281/1500 | Reward:  -3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 538318\n",
      "Episode  282/1500 | Reward:  -3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 545744\n",
      "Episode  283/1500 | Reward:  -7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 551759\n",
      "Episode  284/1500 | Reward:   1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 559161\n",
      "Episode  285/1500 | Reward:   2.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 569161\n",
      "Episode  286/1500 | Reward:  -6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 575460\n",
      "Episode  287/1500 | Reward:   1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 582869\n",
      "Episode  288/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 591985\n",
      "Episode  289/1500 | Reward:  -9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 598523\n",
      "Episode  290/1500 | Reward:  -3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 607752\n",
      "Episode  291/1500 | Reward:   2.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 616798\n",
      "Episode  292/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 623916\n",
      "Episode  293/1500 | Reward:   3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 630765\n",
      "Episode  294/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 637855\n",
      "Episode  295/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 642007\n",
      "Episode  296/1500 | Reward:   1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 649281\n",
      "Episode  297/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 657301\n",
      "Episode  298/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 662536\n",
      "Episode  299/1500 | Reward:  -4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 668476\n",
      "Episode  300/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 676235\n",
      "Episode  301/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 683303\n",
      "Episode  302/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 688065\n",
      "Episode  303/1500 | Reward:   3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 694857\n",
      "Episode  304/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 700732\n",
      "Episode  305/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 707241\n",
      "Episode  306/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 713898\n",
      "Episode  307/1500 | Reward: -14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 718646\n",
      "Episode  308/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 727223\n",
      "Episode  309/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 733036\n",
      "Episode  310/1500 | Reward:  -1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 739982\n",
      "Episode  311/1500 | Reward:  -8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 745553\n",
      "Episode  312/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 751082\n",
      "Episode  313/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 755683\n",
      "Episode  314/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 763032\n",
      "Episode  315/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 768375\n",
      "Episode  316/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 773569\n",
      "Episode  317/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 778456\n",
      "Episode  318/1500 | Reward:  -4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 785599\n",
      "Episode  319/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 790777\n",
      "Episode  320/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 794207\n",
      "Episode  321/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 798605\n",
      "Episode  322/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 802782\n",
      "Episode  323/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 807089\n",
      "Episode  324/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 811368\n",
      "Episode  325/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 815887\n",
      "Episode  326/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 818882\n",
      "Episode  327/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 821401\n",
      "Episode  328/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 823854\n",
      "Episode  329/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 825849\n",
      "Episode  330/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 828346\n",
      "Episode  331/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 831975\n",
      "Episode  332/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 834958\n",
      "Episode  333/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 838267\n",
      "Episode  334/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 841433\n",
      "Episode  335/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 844718\n",
      "Episode  336/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 847453\n",
      "Episode  337/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 851889\n",
      "Episode  338/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 854843\n",
      "Episode  339/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 858909\n",
      "Episode  340/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 861954\n",
      "Episode  341/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 865943\n",
      "Episode  342/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 869784\n",
      "Episode  343/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 873111\n",
      "Episode  344/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 875799\n",
      "Episode  345/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 879136\n",
      "Episode  346/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 881705\n",
      "Episode  347/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 885878\n",
      "Episode  348/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 888063\n",
      "Episode  349/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 891218\n",
      "Episode  350/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 894430\n",
      "Episode  351/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 897864\n",
      "Episode  352/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 901463\n",
      "Episode  353/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 907125\n",
      "Episode  354/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 910440\n",
      "Episode  355/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 914943\n",
      "Episode  356/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 920181\n",
      "Episode  357/1500 | Reward:   5.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 924730\n",
      "Episode  358/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 928485\n",
      "Episode  359/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 931959\n",
      "Episode  360/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 935729\n",
      "Episode  361/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 940520\n",
      "Episode  362/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 944571\n",
      "Episode  363/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 946961\n",
      "Episode  364/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 949406\n",
      "Episode  365/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 952673\n",
      "Episode  366/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 957612\n",
      "Episode  367/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 960314\n",
      "Episode  368/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 963615\n",
      "Episode  369/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 967213\n",
      "Episode  370/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 971094\n",
      "Episode  371/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 973772\n",
      "Episode  372/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 978431\n",
      "Episode  373/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 981830\n",
      "Episode  374/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 985779\n",
      "Episode  375/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 988803\n",
      "Episode  376/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 991854\n",
      "Episode  377/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 994933\n",
      "Episode  378/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 997910\n",
      "Episode  379/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1001024\n",
      "Episode  380/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1004575\n",
      "Episode  381/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1008146\n",
      "Episode  382/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1011954\n",
      "Episode  383/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1015369\n",
      "Episode  384/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1020440\n",
      "Episode  385/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1023597\n",
      "Episode  386/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1026000\n",
      "Episode  387/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1029603\n",
      "Episode  388/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1032988\n",
      "Episode  389/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1035879\n",
      "Episode  390/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1039087\n",
      "Episode  391/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1043442\n",
      "Episode  392/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1047950\n",
      "Episode  393/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1050990\n",
      "Episode  394/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1054773\n",
      "Episode  395/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1058380\n",
      "Episode  396/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1062183\n",
      "Episode  397/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1064645\n",
      "Episode  398/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1068336\n",
      "Episode  399/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1071267\n",
      "Episode  400/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1073884\n",
      "Episode  401/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1076941\n",
      "Episode  402/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1082485\n",
      "Episode  403/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1086464\n",
      "Episode  404/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1091169\n",
      "Episode  405/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1095198\n",
      "Episode  406/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1098963\n",
      "Episode  407/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1102586\n",
      "Episode  408/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1105538\n",
      "Episode  409/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1107761\n",
      "Episode  410/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1110258\n",
      "Episode  411/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1114664\n",
      "Episode  412/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1117883\n",
      "Episode  413/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1120882\n",
      "Episode  414/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1124352\n",
      "Episode  415/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1127928\n",
      "Episode  416/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1131494\n",
      "Episode  417/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1134662\n",
      "Episode  418/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1137706\n",
      "Episode  419/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1141799\n",
      "Episode  420/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1146827\n",
      "Episode  421/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1152322\n",
      "Episode  422/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1154867\n",
      "Episode  423/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1158442\n",
      "Episode  424/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1163412\n",
      "Episode  425/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1166997\n",
      "Episode  426/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1170142\n",
      "Episode  427/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1173193\n",
      "Episode  428/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1175449\n",
      "Episode  429/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1178480\n",
      "Episode  430/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1182130\n",
      "Episode  431/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1186325\n",
      "Episode  432/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1191446\n",
      "Episode  433/1500 | Reward:   3.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1195216\n",
      "Episode  434/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1197545\n",
      "Episode  435/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1200490\n",
      "Episode  436/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1204053\n",
      "Episode  437/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1207667\n",
      "Episode  438/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1211664\n",
      "Episode  439/1500 | Reward:   2.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1217884\n",
      "Episode  440/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1222304\n",
      "Episode  441/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1227685\n",
      "Episode  442/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1229835\n",
      "Episode  443/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1233795\n",
      "Episode  444/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1238423\n",
      "Episode  445/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1241824\n",
      "Episode  446/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1245197\n",
      "Episode  447/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1247408\n",
      "Episode  448/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1251275\n",
      "Episode  449/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1253853\n",
      "Episode  450/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1257759\n",
      "Episode  451/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1260893\n",
      "Episode  452/1500 | Reward:   2.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1266388\n",
      "Episode  453/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1269866\n",
      "Episode  454/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1273399\n",
      "Episode  455/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1276595\n",
      "Episode  456/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1280001\n",
      "Episode  457/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1283320\n",
      "Episode  458/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1286435\n",
      "Episode  459/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1290287\n",
      "Episode  460/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1293491\n",
      "Episode  461/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1297939\n",
      "Episode  462/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1302453\n",
      "Episode  463/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1307100\n",
      "Episode  464/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1310995\n",
      "Episode  465/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1313482\n",
      "Episode  466/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1317323\n",
      "Episode  467/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1321311\n",
      "Episode  468/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1324227\n",
      "Episode  469/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1327924\n",
      "Episode  470/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1332081\n",
      "Episode  471/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1335910\n",
      "Episode  472/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1338891\n",
      "Episode  473/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1342342\n",
      "Episode  474/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1345224\n",
      "Episode  475/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1347290\n",
      "Episode  476/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1350246\n",
      "Episode  477/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1353769\n",
      "Episode  478/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1357144\n",
      "Episode  479/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1360121\n",
      "Episode  480/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1363837\n",
      "Episode  481/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1367213\n",
      "Episode  482/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1369645\n",
      "Episode  483/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1372845\n",
      "Episode  484/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1375450\n",
      "Episode  485/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1378275\n",
      "Episode  486/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1381541\n",
      "Episode  487/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1385254\n",
      "Episode  488/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1387969\n",
      "Episode  489/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1391928\n",
      "Episode  490/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1395105\n",
      "Episode  491/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1398556\n",
      "Episode  492/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1401340\n",
      "Episode  493/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1404366\n",
      "Episode  494/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1408351\n",
      "Episode  495/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1411316\n",
      "Episode  496/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1414514\n",
      "Episode  497/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1417215\n",
      "Episode  498/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1420612\n",
      "Episode  499/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1423317\n",
      "Episode  500/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1426797\n",
      "Episode  501/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1430355\n",
      "Episode  502/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1433864\n",
      "Episode  503/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1438026\n",
      "Episode  504/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1441337\n",
      "Episode  505/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1444816\n",
      "Episode  506/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1448305\n",
      "Episode  507/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1452015\n",
      "Episode  508/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1454500\n",
      "Episode  509/1500 | Reward:   1.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1458888\n",
      "Episode  510/1500 | Reward:   7.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1463543\n",
      "Episode  511/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1466579\n",
      "Episode  512/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1470772\n",
      "Episode  513/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1473693\n",
      "Episode  514/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1477195\n",
      "Episode  515/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1480202\n",
      "Episode  516/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1483303\n",
      "Episode  517/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1485923\n",
      "Episode  518/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1489035\n",
      "Episode  519/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1491390\n",
      "Episode  520/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1494077\n",
      "Episode  521/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1496686\n",
      "Episode  522/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1499924\n",
      "Episode  523/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1504336\n",
      "Episode  524/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1507595\n",
      "Episode  525/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1510968\n",
      "Episode  526/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1514212\n",
      "Episode  527/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1516894\n",
      "Episode  528/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1519764\n",
      "Episode  529/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1522194\n",
      "Episode  530/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1524611\n",
      "Episode  531/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1527335\n",
      "Episode  532/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1530606\n",
      "Episode  533/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1533200\n",
      "Episode  534/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1536038\n",
      "Episode  535/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1539657\n",
      "Episode  536/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1542386\n",
      "Episode  537/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1545273\n",
      "Episode  538/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1547591\n",
      "Episode  539/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1549992\n",
      "Episode  540/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1552467\n",
      "Episode  541/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1555010\n",
      "Episode  542/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1558793\n",
      "Episode  543/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1561977\n",
      "Episode  544/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1564702\n",
      "Episode  545/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1567791\n",
      "Episode  546/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1570997\n",
      "Episode  547/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1573161\n",
      "Episode  548/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1576370\n",
      "Episode  549/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1578819\n",
      "Episode  550/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1581489\n",
      "Episode  551/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1584184\n",
      "Episode  552/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1586891\n",
      "Episode  553/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1590554\n",
      "Episode  554/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1593941\n",
      "Episode  555/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1597087\n",
      "Episode  556/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1599989\n",
      "Episode  557/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1603377\n",
      "Episode  558/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1606035\n",
      "Episode  559/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1609437\n",
      "Episode  560/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1611597\n",
      "Episode  561/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1614481\n",
      "Episode  562/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1617096\n",
      "Episode  563/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1620025\n",
      "Episode  564/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1622418\n",
      "Episode  565/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1625772\n",
      "Episode  566/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1628061\n",
      "Episode  567/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1630512\n",
      "Episode  568/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1633883\n",
      "Episode  569/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1637833\n",
      "Episode  570/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1640032\n",
      "Episode  571/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1644605\n",
      "Episode  572/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1647515\n",
      "Episode  573/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1650443\n",
      "Episode  574/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1652938\n",
      "Episode  575/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1657151\n",
      "Episode  576/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1661790\n",
      "Episode  577/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1664664\n",
      "Episode  578/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1667655\n",
      "Episode  579/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1671077\n",
      "Episode  580/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1673614\n",
      "Episode  581/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1676931\n",
      "Episode  582/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1679915\n",
      "Episode  583/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1682653\n",
      "Episode  584/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1685504\n",
      "Episode  585/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1687605\n",
      "Episode  586/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1690851\n",
      "Episode  587/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1693888\n",
      "Episode  588/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1696491\n",
      "Episode  589/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1699090\n",
      "Episode  590/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1702346\n",
      "Episode  591/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1704946\n",
      "Episode  592/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1707713\n",
      "Episode  593/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1710849\n",
      "Episode  594/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1713850\n",
      "Episode  595/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1716170\n",
      "Episode  596/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1719369\n",
      "Episode  597/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1721887\n",
      "Episode  598/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1724456\n",
      "Episode  599/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1727369\n",
      "Episode  600/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1730325\n",
      "Episode  601/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1733147\n",
      "Episode  602/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1736087\n",
      "Episode  603/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1738689\n",
      "Episode  604/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1741643\n",
      "Episode  605/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1744794\n",
      "Episode  606/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1747654\n",
      "Episode  607/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1750259\n",
      "Episode  608/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1752957\n",
      "Episode  609/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1755809\n",
      "Episode  610/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1759104\n",
      "Episode  611/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1762670\n",
      "Episode  612/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1765359\n",
      "Episode  613/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1768741\n",
      "Episode  614/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1771122\n",
      "Episode  615/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1773631\n",
      "Episode  616/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1776273\n",
      "Episode  617/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1779681\n",
      "Episode  618/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1782496\n",
      "Episode  619/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1785525\n",
      "Episode  620/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1788360\n",
      "Episode  621/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1792221\n",
      "Episode  622/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1795710\n",
      "Episode  623/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1799359\n",
      "Episode  624/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1801607\n",
      "Episode  625/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1805616\n",
      "Episode  626/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1809539\n",
      "Episode  627/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1813435\n",
      "Episode  628/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1816165\n",
      "Episode  629/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1819694\n",
      "Episode  630/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1823481\n",
      "Episode  631/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1827123\n",
      "Episode  632/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1830374\n",
      "Episode  633/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1834418\n",
      "Episode  634/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1836807\n",
      "Episode  635/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1839914\n",
      "Episode  636/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1842739\n",
      "Episode  637/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1845388\n",
      "Episode  638/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1847849\n",
      "Episode  639/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1851245\n",
      "Episode  640/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1854730\n",
      "Episode  641/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1857769\n",
      "Episode  642/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1860173\n",
      "Episode  643/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1863228\n",
      "Episode  644/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1866127\n",
      "Episode  645/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1868788\n",
      "Episode  646/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1871543\n",
      "Episode  647/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1875897\n",
      "Episode  648/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1878208\n",
      "Episode  649/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1880729\n",
      "Episode  650/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1883632\n",
      "Episode  651/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1886919\n",
      "Episode  652/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1891600\n",
      "Episode  653/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1894195\n",
      "Episode  654/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1897184\n",
      "Episode  655/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1901900\n",
      "Episode  656/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1904252\n",
      "Episode  657/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1907388\n",
      "Episode  658/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1910277\n",
      "Episode  659/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1913621\n",
      "Episode  660/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1917152\n",
      "Episode  661/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1919367\n",
      "Episode  662/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1921688\n",
      "Episode  663/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1924691\n",
      "Episode  664/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1927753\n",
      "Episode  665/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1929940\n",
      "Episode  666/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1931935\n",
      "Episode  667/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1935191\n",
      "Episode  668/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1938251\n",
      "Episode  669/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1940675\n",
      "Episode  670/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1942802\n",
      "Episode  671/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1945365\n",
      "Episode  672/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1948240\n",
      "Episode  673/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1951319\n",
      "Episode  674/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1954314\n",
      "Episode  675/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1957016\n",
      "Episode  676/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1959790\n",
      "Episode  677/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1962749\n",
      "Episode  678/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1965208\n",
      "Episode  679/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1968137\n",
      "Episode  680/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1970334\n",
      "Episode  681/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1972868\n",
      "Episode  682/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1975637\n",
      "Episode  683/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1977692\n",
      "Episode  684/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1980346\n",
      "Episode  685/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1983709\n",
      "Episode  686/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1985872\n",
      "Episode  687/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1988735\n",
      "Episode  688/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1991739\n",
      "Episode  689/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1994034\n",
      "Episode  690/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1996089\n",
      "Episode  691/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 1998397\n",
      "Episode  692/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2000932\n",
      "Episode  693/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2003406\n",
      "Episode  694/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2005879\n",
      "Episode  695/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2009073\n",
      "Episode  696/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2012471\n",
      "Episode  697/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2015225\n",
      "Episode  698/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2017363\n",
      "Episode  699/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2020468\n",
      "Episode  700/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2023460\n",
      "Episode  701/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2025801\n",
      "Episode  702/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2028284\n",
      "Episode  703/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2031397\n",
      "Episode  704/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2034049\n",
      "Episode  705/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2036358\n",
      "Episode  706/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2038928\n",
      "Episode  707/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2041219\n",
      "Episode  708/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2044287\n",
      "Episode  709/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2047804\n",
      "Episode  710/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2050440\n",
      "Episode  711/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2052723\n",
      "Episode  712/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2055414\n",
      "Episode  713/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2058577\n",
      "Episode  714/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2061534\n",
      "Episode  715/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2064229\n",
      "Episode  716/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2066391\n",
      "Episode  717/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2069194\n",
      "Episode  718/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2071693\n",
      "Episode  719/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2074600\n",
      "Episode  720/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2077445\n",
      "Episode  721/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2080330\n",
      "Episode  722/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2082493\n",
      "Episode  723/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2085945\n",
      "Episode  724/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2088936\n",
      "Episode  725/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2091771\n",
      "Episode  726/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2095614\n",
      "Episode  727/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2097669\n",
      "Episode  728/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2099700\n",
      "Episode  729/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2103105\n",
      "Episode  730/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2106923\n",
      "Episode  731/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2110672\n",
      "Episode  732/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2112764\n",
      "Episode  733/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2114995\n",
      "Episode  734/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2117167\n",
      "Episode  735/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2119944\n",
      "Episode  736/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2122439\n",
      "Episode  737/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2124975\n",
      "Episode  738/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2127614\n",
      "Episode  739/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2131015\n",
      "Episode  740/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2133422\n",
      "Episode  741/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2135725\n",
      "Episode  742/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2138834\n",
      "Episode  743/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2141839\n",
      "Episode  744/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2144616\n",
      "Episode  745/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2147185\n",
      "Episode  746/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2150452\n",
      "Episode  747/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2153349\n",
      "Episode  748/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2156581\n",
      "Episode  749/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2159558\n",
      "Episode  750/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2161648\n",
      "Episode  751/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2164016\n",
      "Episode  752/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2166405\n",
      "Episode  753/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2169206\n",
      "Episode  754/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2171707\n",
      "Episode  755/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2174255\n",
      "Episode  756/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2176729\n",
      "Episode  757/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2179624\n",
      "Episode  758/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2182241\n",
      "Episode  759/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2184632\n",
      "Episode  760/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2187890\n",
      "Episode  761/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2190025\n",
      "Episode  762/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2192198\n",
      "Episode  763/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2194555\n",
      "Episode  764/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2197382\n",
      "Episode  765/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2200310\n",
      "Episode  766/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2203251\n",
      "Episode  767/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2205593\n",
      "Episode  768/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2208595\n",
      "Episode  769/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2211400\n",
      "Episode  770/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2214385\n",
      "Episode  771/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2216572\n",
      "Episode  772/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2219698\n",
      "Episode  773/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2222563\n",
      "Episode  774/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2225449\n",
      "Episode  775/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2227840\n",
      "Episode  776/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2229907\n",
      "Episode  777/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2232988\n",
      "Episode  778/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2235788\n",
      "Episode  779/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2237879\n",
      "Episode  780/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2240160\n",
      "Episode  781/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2243705\n",
      "Episode  782/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2245902\n",
      "Episode  783/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2249751\n",
      "Episode  784/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2252082\n",
      "Episode  785/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2254665\n",
      "Episode  786/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2256974\n",
      "Episode  787/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2259572\n",
      "Episode  788/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2261841\n",
      "Episode  789/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2264591\n",
      "Episode  790/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2267363\n",
      "Episode  791/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2269742\n",
      "Episode  792/1500 | Reward:   2.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2273254\n",
      "Episode  793/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2275753\n",
      "Episode  794/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2277954\n",
      "Episode  795/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2280657\n",
      "Episode  796/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2284084\n",
      "Episode  797/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2286741\n",
      "Episode  798/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2289867\n",
      "Episode  799/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2293209\n",
      "Episode  800/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2296306\n",
      "Episode  801/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2300400\n",
      "Episode  802/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2303021\n",
      "Episode  803/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2305835\n",
      "Episode  804/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2309182\n",
      "Episode  805/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2312003\n",
      "Episode  806/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2315730\n",
      "Episode  807/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2318963\n",
      "Episode  808/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2322369\n",
      "Episode  809/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2326028\n",
      "Episode  810/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2329029\n",
      "Episode  811/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2332112\n",
      "Episode  812/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2334909\n",
      "Episode  813/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2337924\n",
      "Episode  814/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2340254\n",
      "Episode  815/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2342484\n",
      "Episode  816/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2345855\n",
      "Episode  817/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2348596\n",
      "Episode  818/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2351047\n",
      "Episode  819/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2353102\n",
      "Episode  820/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2355775\n",
      "Episode  821/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2358408\n",
      "Episode  822/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2360965\n",
      "Episode  823/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2363150\n",
      "Episode  824/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2365747\n",
      "Episode  825/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2368014\n",
      "Episode  826/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2370948\n",
      "Episode  827/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2373793\n",
      "Episode  828/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2376953\n",
      "Episode  829/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2379238\n",
      "Episode  830/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2381927\n",
      "Episode  831/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2384149\n",
      "Episode  832/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2386698\n",
      "Episode  833/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2389227\n",
      "Episode  834/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2391645\n",
      "Episode  835/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2394387\n",
      "Episode  836/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2396660\n",
      "Episode  837/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2399556\n",
      "Episode  838/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2402125\n",
      "Episode  839/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2405151\n",
      "Episode  840/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2407349\n",
      "Episode  841/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2409751\n",
      "Episode  842/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2412333\n",
      "Episode  843/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2414890\n",
      "Episode  844/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2417731\n",
      "Episode  845/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2421216\n",
      "Episode  846/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2423693\n",
      "Episode  847/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2426024\n",
      "Episode  848/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2429150\n",
      "Episode  849/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2431252\n",
      "Episode  850/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2434258\n",
      "Episode  851/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2437063\n",
      "Episode  852/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2439350\n",
      "Episode  853/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2441975\n",
      "Episode  854/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2444068\n",
      "Episode  855/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2446503\n",
      "Episode  856/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2448820\n",
      "Episode  857/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2451405\n",
      "Episode  858/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2453759\n",
      "Episode  859/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2456054\n",
      "Episode  860/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2458398\n",
      "Episode  861/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2461295\n",
      "Episode  862/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2463527\n",
      "Episode  863/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2466113\n",
      "Episode  864/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2468394\n",
      "Episode  865/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2470495\n",
      "Episode  866/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2472694\n",
      "Episode  867/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2476043\n",
      "Episode  868/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2479335\n",
      "Episode  869/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2481690\n",
      "Episode  870/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2484117\n",
      "Episode  871/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2486842\n",
      "Episode  872/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2489634\n",
      "Episode  873/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2493036\n",
      "Episode  874/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2495911\n",
      "Episode  875/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2498872\n",
      "Episode  876/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2501107\n",
      "Episode  877/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2504248\n",
      "Episode  878/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2506653\n",
      "Episode  879/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2509467\n",
      "Episode  880/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2511874\n",
      "Episode  881/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2513941\n",
      "Episode  882/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2516298\n",
      "Episode  883/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2518521\n",
      "Episode  884/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2520728\n",
      "Episode  885/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2522857\n",
      "Episode  886/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2524852\n",
      "Episode  887/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2527654\n",
      "Episode  888/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2529649\n",
      "Episode  889/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2531970\n",
      "Episode  890/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2535057\n",
      "Episode  891/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2537324\n",
      "Episode  892/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2539741\n",
      "Episode  893/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2541856\n",
      "Episode  894/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2544285\n",
      "Episode  895/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2547290\n",
      "Episode  896/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2549451\n",
      "Episode  897/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2552207\n",
      "Episode  898/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2554489\n",
      "Episode  899/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2558558\n",
      "Episode  900/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2561537\n",
      "Episode  901/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2563770\n",
      "Episode  902/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2566087\n",
      "Episode  903/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2568659\n",
      "Episode  904/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2570785\n",
      "Episode  905/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2573680\n",
      "Episode  906/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2576049\n",
      "Episode  907/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2579074\n",
      "Episode  908/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2581299\n",
      "Episode  909/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2583580\n",
      "Episode  910/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2586307\n",
      "Episode  911/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2590021\n",
      "Episode  912/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2592146\n",
      "Episode  913/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2594320\n",
      "Episode  914/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2596675\n",
      "Episode  915/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2599044\n",
      "Episode  916/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2601293\n",
      "Episode  917/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2604159\n",
      "Episode  918/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2606528\n",
      "Episode  919/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2608657\n",
      "Episode  920/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2611145\n",
      "Episode  921/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2613666\n",
      "Episode  922/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2616191\n",
      "Episode  923/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2618724\n",
      "Episode  924/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2621043\n",
      "Episode  925/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2624869\n",
      "Episode  926/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2627101\n",
      "Episode  927/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2629854\n",
      "Episode  928/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2632695\n",
      "Episode  929/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2634848\n",
      "Episode  930/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2637083\n",
      "Episode  931/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2639704\n",
      "Episode  932/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2641893\n",
      "Episode  933/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2643984\n",
      "Episode  934/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2646367\n",
      "Episode  935/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2649116\n",
      "Episode  936/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2652070\n",
      "Episode  937/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2654545\n",
      "Episode  938/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2657092\n",
      "Episode  939/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2659934\n",
      "Episode  940/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2662037\n",
      "Episode  941/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2664722\n",
      "Episode  942/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2666861\n",
      "Episode  943/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2669963\n",
      "Episode  944/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2672449\n",
      "Episode  945/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2676105\n",
      "Episode  946/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2678760\n",
      "Episode  947/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2681413\n",
      "Episode  948/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2683832\n",
      "Episode  949/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2686731\n",
      "Episode  950/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2688834\n",
      "Episode  951/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2691321\n",
      "Episode  952/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2693712\n",
      "Episode  953/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2696081\n",
      "Episode  954/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2699215\n",
      "Episode  955/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2701451\n",
      "Episode  956/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2704046\n",
      "Episode  957/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2706519\n",
      "Episode  958/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2709960\n",
      "Episode  959/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2713563\n",
      "Episode  960/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2716195\n",
      "Episode  961/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2718420\n",
      "Episode  962/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2721754\n",
      "Episode  963/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2724549\n",
      "Episode  964/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2727484\n",
      "Episode  965/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2729685\n",
      "Episode  966/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2731967\n",
      "Episode  967/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2734290\n",
      "Episode  968/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2736695\n",
      "Episode  969/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2739705\n",
      "Episode  970/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2741936\n",
      "Episode  971/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2744135\n",
      "Episode  972/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2746332\n",
      "Episode  973/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2748689\n",
      "Episode  974/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2751043\n",
      "Episode  975/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2753612\n",
      "Episode  976/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2756390\n",
      "Episode  977/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2758602\n",
      "Episode  978/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2760871\n",
      "Episode  979/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2763234\n",
      "Episode  980/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2765613\n",
      "Episode  981/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2768302\n",
      "Episode  982/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2770725\n",
      "Episode  983/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2773193\n",
      "Episode  984/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2775418\n",
      "Episode  985/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2777919\n",
      "Episode  986/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2780970\n",
      "Episode  987/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2783871\n",
      "Episode  988/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2786310\n",
      "Episode  989/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2788628\n",
      "Episode  990/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2791352\n",
      "Episode  991/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2793540\n",
      "Episode  992/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2796412\n",
      "Episode  993/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2798514\n",
      "Episode  994/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2800545\n",
      "Episode  995/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2802888\n",
      "Episode  996/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2805663\n",
      "Episode  997/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2807982\n",
      "Episode  998/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2810195\n",
      "Episode  999/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2812523\n",
      "Episode 1000/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2815343\n",
      "Episode 1001/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2817722\n",
      "Episode 1002/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2820450\n",
      "Episode 1003/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2823240\n",
      "Episode 1004/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2825501\n",
      "Episode 1005/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2827794\n",
      "Episode 1006/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2830235\n",
      "Episode 1007/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2832818\n",
      "Episode 1008/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2834813\n",
      "Episode 1009/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2837719\n",
      "Episode 1010/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2840606\n",
      "Episode 1011/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2843073\n",
      "Episode 1012/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2845447\n",
      "Episode 1013/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2849295\n",
      "Episode 1014/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2852470\n",
      "Episode 1015/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2855161\n",
      "Episode 1016/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2858064\n",
      "Episode 1017/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2860651\n",
      "Episode 1018/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2863942\n",
      "Episode 1019/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2866457\n",
      "Episode 1020/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2868690\n",
      "Episode 1021/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2871072\n",
      "Episode 1022/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2875427\n",
      "Episode 1023/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2878000\n",
      "Episode 1024/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2880500\n",
      "Episode 1025/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2882733\n",
      "Episode 1026/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2884895\n",
      "Episode 1027/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2887477\n",
      "Episode 1028/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2890134\n",
      "Episode 1029/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2892631\n",
      "Episode 1030/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2894891\n",
      "Episode 1031/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2897255\n",
      "Episode 1032/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2899523\n",
      "Episode 1033/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2902531\n",
      "Episode 1034/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2905116\n",
      "Episode 1035/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2908661\n",
      "Episode 1036/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2911001\n",
      "Episode 1037/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2913244\n",
      "Episode 1038/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2915644\n",
      "Episode 1039/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2917999\n",
      "Episode 1040/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2920629\n",
      "Episode 1041/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2923127\n",
      "Episode 1042/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2925567\n",
      "Episode 1043/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2927824\n",
      "Episode 1044/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2930503\n",
      "Episode 1045/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2933497\n",
      "Episode 1046/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2935805\n",
      "Episode 1047/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2938026\n",
      "Episode 1048/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2941244\n",
      "Episode 1049/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2944169\n",
      "Episode 1050/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2946272\n",
      "Episode 1051/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2948373\n",
      "Episode 1052/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2950902\n",
      "Episode 1053/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2953851\n",
      "Episode 1054/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2957488\n",
      "Episode 1055/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2959553\n",
      "Episode 1056/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2962209\n",
      "Episode 1057/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2964372\n",
      "Episode 1058/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2966787\n",
      "Episode 1059/1500 | Reward:   4.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2971084\n",
      "Episode 1060/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2973694\n",
      "Episode 1061/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2976919\n",
      "Episode 1062/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2979239\n",
      "Episode 1063/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2982599\n",
      "Episode 1064/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2984852\n",
      "Episode 1065/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2987290\n",
      "Episode 1066/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2990043\n",
      "Episode 1067/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2992532\n",
      "Episode 1068/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2994765\n",
      "Episode 1069/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2997285\n",
      "Episode 1070/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 2999904\n",
      "Episode 1071/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3002827\n",
      "Episode 1072/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3005136\n",
      "Episode 1073/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3007620\n",
      "Episode 1074/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3010110\n",
      "Episode 1075/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3012587\n",
      "Episode 1076/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3015632\n",
      "Episode 1077/1500 | Reward:   6.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3018401\n",
      "Episode 1078/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3020770\n",
      "Episode 1079/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3022872\n",
      "Episode 1080/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3025083\n",
      "Episode 1081/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3027208\n",
      "Episode 1082/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3029203\n",
      "Episode 1083/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3031414\n",
      "Episode 1084/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3033887\n",
      "Episode 1085/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3036650\n",
      "Episode 1086/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3039489\n",
      "Episode 1087/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3042182\n",
      "Episode 1088/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3044484\n",
      "Episode 1089/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3046952\n",
      "Episode 1090/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3049824\n",
      "Episode 1091/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3053847\n",
      "Episode 1092/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3056096\n",
      "Episode 1093/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3058839\n",
      "Episode 1094/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3061373\n",
      "Episode 1095/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3063547\n",
      "Episode 1096/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3065744\n",
      "Episode 1097/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3068147\n",
      "Episode 1098/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3071156\n",
      "Episode 1099/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3073680\n",
      "Episode 1100/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3075999\n",
      "Episode 1101/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3078766\n",
      "Episode 1102/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3081124\n",
      "Episode 1103/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3084098\n",
      "Episode 1104/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3086155\n",
      "Episode 1105/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3088818\n",
      "Episode 1106/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3091067\n",
      "Episode 1107/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3093748\n",
      "Episode 1108/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3097717\n",
      "Episode 1109/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3100240\n",
      "Episode 1110/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3102715\n",
      "Episode 1111/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3104988\n",
      "Episode 1112/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3107501\n",
      "Episode 1113/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3109724\n",
      "Episode 1114/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3111791\n",
      "Episode 1115/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3114172\n",
      "Episode 1116/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3117343\n",
      "Episode 1117/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3119723\n",
      "Episode 1118/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3121886\n",
      "Episode 1119/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3124445\n",
      "Episode 1120/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3127025\n",
      "Episode 1121/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3129461\n",
      "Episode 1122/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3131756\n",
      "Episode 1123/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3135016\n",
      "Episode 1124/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3137639\n",
      "Episode 1125/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3141408\n",
      "Episode 1126/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3145111\n",
      "Episode 1127/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3147510\n",
      "Episode 1128/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3149733\n",
      "Episode 1129/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3152715\n",
      "Episode 1130/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3155678\n",
      "Episode 1131/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3158920\n",
      "Episode 1132/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3161746\n",
      "Episode 1133/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3164407\n",
      "Episode 1134/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3166593\n",
      "Episode 1135/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3169407\n",
      "Episode 1136/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3171717\n",
      "Episode 1137/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3173878\n",
      "Episode 1138/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3175980\n",
      "Episode 1139/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3179918\n",
      "Episode 1140/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3183517\n",
      "Episode 1141/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3185848\n",
      "Episode 1142/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3188227\n",
      "Episode 1143/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3190596\n",
      "Episode 1144/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3192731\n",
      "Episode 1145/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3195341\n",
      "Episode 1146/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3197444\n",
      "Episode 1147/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3199511\n",
      "Episode 1148/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3202306\n",
      "Episode 1149/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3205111\n",
      "Episode 1150/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3207388\n",
      "Episode 1151/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3209996\n",
      "Episode 1152/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3212288\n",
      "Episode 1153/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3214669\n",
      "Episode 1154/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3217884\n",
      "Episode 1155/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3220010\n",
      "Episode 1156/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3222307\n",
      "Episode 1157/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3224786\n",
      "Episode 1158/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3227009\n",
      "Episode 1159/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3230342\n",
      "Episode 1160/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3232940\n",
      "Episode 1161/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3235755\n",
      "Episode 1162/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3238379\n",
      "Episode 1163/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3240508\n",
      "Episode 1164/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3242694\n",
      "Episode 1165/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3245301\n",
      "Episode 1166/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3248475\n",
      "Episode 1167/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3251142\n",
      "Episode 1168/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3253137\n",
      "Episode 1169/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3255826\n",
      "Episode 1170/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3257928\n",
      "Episode 1171/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3260128\n",
      "Episode 1172/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3262795\n",
      "Episode 1173/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3264898\n",
      "Episode 1174/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3267387\n",
      "Episode 1175/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3270076\n",
      "Episode 1176/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3272995\n",
      "Episode 1177/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3275350\n",
      "Episode 1178/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3278303\n",
      "Episode 1179/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3280403\n",
      "Episode 1180/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3282688\n",
      "Episode 1181/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3285215\n",
      "Episode 1182/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3287628\n",
      "Episode 1183/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3289801\n",
      "Episode 1184/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3291868\n",
      "Episode 1185/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3294293\n",
      "Episode 1186/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3296491\n",
      "Episode 1187/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3298718\n",
      "Episode 1188/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3301048\n",
      "Episode 1189/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3303275\n",
      "Episode 1190/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3305826\n",
      "Episode 1191/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3308024\n",
      "Episode 1192/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3310189\n",
      "Episode 1193/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3312256\n",
      "Episode 1194/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3315003\n",
      "Episode 1195/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3318254\n",
      "Episode 1196/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3320735\n",
      "Episode 1197/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3324172\n",
      "Episode 1198/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3326585\n",
      "Episode 1199/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3329071\n",
      "Episode 1200/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3332485\n",
      "Episode 1201/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3335421\n",
      "Episode 1202/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3337594\n",
      "Episode 1203/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3339735\n",
      "Episode 1204/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3342701\n",
      "Episode 1205/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3345301\n",
      "Episode 1206/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3347957\n",
      "Episode 1207/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3350505\n",
      "Episode 1208/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3353308\n",
      "Episode 1209/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3355531\n",
      "Episode 1210/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3357900\n",
      "Episode 1211/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3360883\n",
      "Episode 1212/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3363238\n",
      "Episode 1213/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3365783\n",
      "Episode 1214/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3367946\n",
      "Episode 1215/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3370013\n",
      "Episode 1216/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3372246\n",
      "Episode 1217/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3375267\n",
      "Episode 1218/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3378296\n",
      "Episode 1219/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3380943\n",
      "Episode 1220/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3383273\n",
      "Episode 1221/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3386305\n",
      "Episode 1222/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3389007\n",
      "Episode 1223/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3391590\n",
      "Episode 1224/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3394437\n",
      "Episode 1225/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3396505\n",
      "Episode 1226/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3398668\n",
      "Episode 1227/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3401011\n",
      "Episode 1228/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3403270\n",
      "Episode 1229/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3405646\n",
      "Episode 1230/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3408105\n",
      "Episode 1231/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3410922\n",
      "Episode 1232/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3413131\n",
      "Episode 1233/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3415416\n",
      "Episode 1234/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3417810\n",
      "Episode 1235/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3420041\n",
      "Episode 1236/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3422335\n",
      "Episode 1237/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3424835\n",
      "Episode 1238/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3427475\n",
      "Episode 1239/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3431082\n",
      "Episode 1240/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3433927\n",
      "Episode 1241/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3436030\n",
      "Episode 1242/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3438971\n",
      "Episode 1243/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3441399\n",
      "Episode 1244/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3444068\n",
      "Episode 1245/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3446451\n",
      "Episode 1246/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3449348\n",
      "Episode 1247/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3452035\n",
      "Episode 1248/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3454851\n",
      "Episode 1249/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3457366\n",
      "Episode 1250/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3460249\n",
      "Episode 1251/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3462906\n",
      "Episode 1252/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3465478\n",
      "Episode 1253/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3467700\n",
      "Episode 1254/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3469874\n",
      "Episode 1255/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3472898\n",
      "Episode 1256/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3475735\n",
      "Episode 1257/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3478661\n",
      "Episode 1258/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3481088\n",
      "Episode 1259/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3483407\n",
      "Episode 1260/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3486356\n",
      "Episode 1261/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3488987\n",
      "Episode 1262/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3491234\n",
      "Episode 1263/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3494176\n",
      "Episode 1264/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3497525\n",
      "Episode 1265/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3500158\n",
      "Episode 1266/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3502285\n",
      "Episode 1267/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3505086\n",
      "Episode 1268/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3507929\n",
      "Episode 1269/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3510863\n",
      "Episode 1270/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3513478\n",
      "Episode 1271/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3516610\n",
      "Episode 1272/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3519543\n",
      "Episode 1273/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3522160\n",
      "Episode 1274/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3524743\n",
      "Episode 1275/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3527182\n",
      "Episode 1276/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3530281\n",
      "Episode 1277/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3533312\n",
      "Episode 1278/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3535639\n",
      "Episode 1279/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3537920\n",
      "Episode 1280/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3541446\n",
      "Episode 1281/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3543513\n",
      "Episode 1282/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3545617\n",
      "Episode 1283/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3548085\n",
      "Episode 1284/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3550259\n",
      "Episode 1285/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3552692\n",
      "Episode 1286/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3555045\n",
      "Episode 1287/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3558022\n",
      "Episode 1288/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3560481\n",
      "Episode 1289/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3562842\n",
      "Episode 1290/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3565283\n",
      "Episode 1291/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3568162\n",
      "Episode 1292/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3570299\n",
      "Episode 1293/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3572598\n",
      "Episode 1294/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3574820\n",
      "Episode 1295/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3576995\n",
      "Episode 1296/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3579516\n",
      "Episode 1297/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3582435\n",
      "Episode 1298/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3585234\n",
      "Episode 1299/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3587605\n",
      "Episode 1300/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3589984\n",
      "Episode 1301/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3592015\n",
      "Episode 1302/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3595476\n",
      "Episode 1303/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3598062\n",
      "Episode 1304/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3600772\n",
      "Episode 1305/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3603056\n",
      "Episode 1306/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3605123\n",
      "Episode 1307/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3607513\n",
      "Episode 1308/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3609963\n",
      "Episode 1309/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3612308\n",
      "Episode 1310/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3614579\n",
      "Episode 1311/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3616874\n",
      "Episode 1312/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3619455\n",
      "Episode 1313/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3621522\n",
      "Episode 1314/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3623867\n",
      "Episode 1315/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3626372\n",
      "Episode 1316/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3628475\n",
      "Episode 1317/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3631240\n",
      "Episode 1318/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3633489\n",
      "Episode 1319/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3636085\n",
      "Episode 1320/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3638890\n",
      "Episode 1321/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3642450\n",
      "Episode 1322/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3644749\n",
      "Episode 1323/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3647222\n",
      "Episode 1324/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3650452\n",
      "Episode 1325/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3652960\n",
      "Episode 1326/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3655192\n",
      "Episode 1327/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3658103\n",
      "Episode 1328/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3660386\n",
      "Episode 1329/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3662453\n",
      "Episode 1330/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3664812\n",
      "Episode 1331/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3667240\n",
      "Episode 1332/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3669608\n",
      "Episode 1333/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3671957\n",
      "Episode 1334/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3675268\n",
      "Episode 1335/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3679229\n",
      "Episode 1336/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3681536\n",
      "Episode 1337/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3685158\n",
      "Episode 1338/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3687607\n",
      "Episode 1339/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3690538\n",
      "Episode 1340/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3693970\n",
      "Episode 1341/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3696407\n",
      "Episode 1342/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3699125\n",
      "Episode 1343/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3701313\n",
      "Episode 1344/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3703801\n",
      "Episode 1345/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3707266\n",
      "Episode 1346/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3710564\n",
      "Episode 1347/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3712987\n",
      "Episode 1348/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3715344\n",
      "Episode 1349/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3717700\n",
      "Episode 1350/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3719931\n",
      "Episode 1351/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3722154\n",
      "Episode 1352/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3724963\n",
      "Episode 1353/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3727162\n",
      "Episode 1354/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3729301\n",
      "Episode 1355/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3731958\n",
      "Episode 1356/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3734423\n",
      "Episode 1357/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3736588\n",
      "Episode 1358/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3739015\n",
      "Episode 1359/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3741613\n",
      "Episode 1360/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3744147\n",
      "Episode 1361/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3746504\n",
      "Episode 1362/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3749077\n",
      "Episode 1363/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3751180\n",
      "Episode 1364/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3754587\n",
      "Episode 1365/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3757130\n",
      "Episode 1366/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3759585\n",
      "Episode 1367/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3761618\n",
      "Episode 1368/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3764619\n",
      "Episode 1369/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3767565\n",
      "Episode 1370/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3770027\n",
      "Episode 1371/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3772406\n",
      "Episode 1372/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3774811\n",
      "Episode 1373/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3777068\n",
      "Episode 1374/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3779754\n",
      "Episode 1375/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3782355\n",
      "Episode 1376/1500 | Reward:  10.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3785470\n",
      "Episode 1377/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3788619\n",
      "Episode 1378/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3791379\n",
      "Episode 1379/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3793832\n",
      "Episode 1380/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3796519\n",
      "Episode 1381/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3799064\n",
      "Episode 1382/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3801323\n",
      "Episode 1383/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3804730\n",
      "Episode 1384/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3807024\n",
      "Episode 1385/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3809125\n",
      "Episode 1386/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3811495\n",
      "Episode 1387/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3813950\n",
      "Episode 1388/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3816125\n",
      "Episode 1389/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3818872\n",
      "Episode 1390/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3821525\n",
      "Episode 1391/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3824597\n",
      "Episode 1392/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3827027\n",
      "Episode 1393/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3830198\n",
      "Episode 1394/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3832675\n",
      "Episode 1395/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3834960\n",
      "Episode 1396/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3837668\n",
      "Episode 1397/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3840129\n",
      "Episode 1398/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3842425\n",
      "Episode 1399/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3845109\n",
      "Episode 1400/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3847710\n",
      "Episode 1401/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3850141\n",
      "Episode 1402/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3852294\n",
      "Episode 1403/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3855063\n",
      "Episode 1404/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3857336\n",
      "Episode 1405/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3859898\n",
      "Episode 1406/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3862121\n",
      "Episode 1407/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3864250\n",
      "Episode 1408/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3867765\n",
      "Episode 1409/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3870012\n",
      "Episode 1410/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3872103\n",
      "Episode 1411/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3874646\n",
      "Episode 1412/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3877389\n",
      "Episode 1413/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3879872\n",
      "Episode 1414/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3881939\n",
      "Episode 1415/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3884618\n",
      "Episode 1416/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3886945\n",
      "Episode 1417/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3889454\n",
      "Episode 1418/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3891485\n",
      "Episode 1419/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3893778\n",
      "Episode 1420/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3896689\n",
      "Episode 1421/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3899110\n",
      "Episode 1422/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3901483\n",
      "Episode 1423/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3904214\n",
      "Episode 1424/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3906580\n",
      "Episode 1425/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3909187\n",
      "Episode 1426/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3912100\n",
      "Episode 1427/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3914277\n",
      "Episode 1428/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3917078\n",
      "Episode 1429/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3919565\n",
      "Episode 1430/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3921852\n",
      "Episode 1431/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3924053\n",
      "Episode 1432/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3926915\n",
      "Episode 1433/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3929730\n",
      "Episode 1434/1500 | Reward:   9.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3933006\n",
      "Episode 1435/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3935521\n",
      "Episode 1436/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3937709\n",
      "Episode 1437/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3940022\n",
      "Episode 1438/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3942185\n",
      "Episode 1439/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3944740\n",
      "Episode 1440/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3946833\n",
      "Episode 1441/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3948828\n",
      "Episode 1442/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3951150\n",
      "Episode 1443/1500 | Reward:  21.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3953205\n",
      "Episode 1444/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3955778\n",
      "Episode 1445/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3958592\n",
      "Episode 1446/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3960931\n",
      "Episode 1447/1500 | Reward:  12.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3964150\n",
      "Episode 1448/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3966555\n",
      "Episode 1449/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3969470\n",
      "Episode 1450/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3971812\n",
      "Episode 1451/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3974679\n",
      "Episode 1452/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3976808\n",
      "Episode 1453/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3979563\n",
      "Episode 1454/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3982282\n",
      "Episode 1455/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3984446\n",
      "Episode 1456/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3986620\n",
      "Episode 1457/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3989107\n",
      "Episode 1458/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3991790\n",
      "Episode 1459/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3994494\n",
      "Episode 1460/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3996724\n",
      "Episode 1461/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 3999330\n",
      "Episode 1462/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4001648\n",
      "Episode 1463/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4004005\n",
      "Episode 1464/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4006906\n",
      "Episode 1465/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4009259\n",
      "Episode 1466/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4011930\n",
      "Episode 1467/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4014568\n",
      "Episode 1468/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4016829\n",
      "Episode 1469/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4019490\n",
      "Episode 1470/1500 | Reward:  16.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4022269\n",
      "Episode 1471/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4025826\n",
      "Episode 1472/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4028432\n",
      "Episode 1473/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4031301\n",
      "Episode 1474/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4033706\n",
      "Episode 1475/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4036163\n",
      "Episode 1476/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4038568\n",
      "Episode 1477/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4041435\n",
      "Episode 1478/1500 | Reward:  20.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4043646\n",
      "Episode 1479/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4046050\n",
      "Episode 1480/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4048237\n",
      "Episode 1481/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4050570\n",
      "Episode 1482/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4053519\n",
      "Episode 1483/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4056387\n",
      "Episode 1484/1500 | Reward:  13.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4059390\n",
      "Episode 1485/1500 | Reward:   8.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4062092\n",
      "Episode 1486/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4064229\n",
      "Episode 1487/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4066514\n",
      "Episode 1488/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4069193\n",
      "Episode 1489/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4071450\n",
      "Episode 1490/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4073842\n",
      "Episode 1491/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4076029\n",
      "Episode 1492/1500 | Reward:  15.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4078787\n",
      "Episode 1493/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4081688\n",
      "Episode 1494/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4084481\n",
      "Episode 1495/1500 | Reward:  17.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4087226\n",
      "Episode 1496/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4089453\n",
      "Episode 1497/1500 | Reward:  14.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4091944\n",
      "Episode 1498/1500 | Reward:  11.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4095087\n",
      "Episode 1499/1500 | Reward:  19.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4097154\n",
      "Episode 1500/1500 | Reward:  18.0 | Epsilon: 0.100 | Buffer: 100000 | Steps: 4099437\n",
      "\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Full Training Loop (Configurable)\n",
    "# ------------------------------------------------------------\n",
    "# This cell trains the DQN for a longer run, e.g., 500+ episodes,\n",
    "# with proper logging and target-network updates.\n",
    "# For the assignment, we will likely run multiple sessions and\n",
    "# compare plots.\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training started at:\", datetime.datetime.now())\n",
    "\n",
    "# Reset counters and storage for a fresh training run.\n",
    "steps_done = 0\n",
    "episode_rewards = []\n",
    "losses = []  # track training losses over time\n",
    "\n",
    "# Training configuration - pull values from our config cell.\n",
    "num_episodes = NUM_EPISODES         \n",
    "target_update_interval = TARGET_UPDATE_INTERVAL  # update target network every N steps\n",
    "max_steps_per_episode = MAX_STEPS_PER_EPISODE   # safety cutoff to avoid infinite loops\n",
    "\n",
    "# Create the replay buffer to store our experiences.\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Main training loop - run for num_episodes episodes.\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    # Reset the environment to start a fresh episode.\n",
    "    obs, info = env.reset()\n",
    "    frame_stack = FrameStack(num_frames=4)\n",
    "    state = frame_stack.reset(obs)\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Run the episode until it ends or we hit the max step limit.\n",
    "    while not done and step < max_steps_per_episode:\n",
    "        step += 1\n",
    "\n",
    "        # Pick an action using epsilon-greedy policy (explore or exploit).\n",
    "        action_index, env_action, epsilon = select_action(state)\n",
    "\n",
    "        # Apply frame skipping: repeat the same action for FRAME_SKIP frames.\n",
    "        # This speeds up training and is standard practice for Atari DQN.\n",
    "        total_reward = 0.0\n",
    "        for _ in range(FRAME_SKIP):\n",
    "            # Step the environment with our chosen action.\n",
    "            next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Clip the reward to [-1, 0, +1] for training stability.\n",
    "            clipped_reward = clip_reward(reward)\n",
    "            total_reward += clipped_reward\n",
    "            \n",
    "            # If the episode ended, stop frame skipping early.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update our frame stack to get the next state.\n",
    "        next_state = frame_stack.step(next_obs)\n",
    "\n",
    "        # Save this experience to the replay buffer (we store the ACTION INDEX).\n",
    "        replay_buffer.push(state, action_index, total_reward, next_state, done)\n",
    "\n",
    "        # Move to the next state.\n",
    "        state = next_state\n",
    "        episode_reward += total_reward\n",
    "\n",
    "        # Only train every N steps to avoid overfitting early on.\n",
    "        # This also speeds up training since we're not updating on every single step.\n",
    "        if steps_done % TRAIN_EVERY_N_STEPS == 0:\n",
    "            loss = optimize_model(replay_buffer, use_double_dqn=USE_DOUBLE_DQN)\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "\n",
    "        # Update the target network periodically (every N steps, not episodes).\n",
    "        # The target network provides stable Q-value estimates during training.\n",
    "        if steps_done % target_update_interval == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Record the total reward for this episode.\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    # Print a summary after each episode so we can track progress.\n",
    "    print(\n",
    "        f\"Episode {episode:4d}/{num_episodes} | \"\n",
    "        f\"Reward: {episode_reward:5.1f} | \"\n",
    "        f\"Epsilon: {get_epsilon(steps_done):.3f} | \"\n",
    "        f\"Buffer: {len(replay_buffer)} | \"\n",
    "        f\"Steps: {steps_done}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86849387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rewards: [np.float64(-21.0), np.float64(-21.0), np.float64(-21.0), np.float64(-21.0), np.float64(-21.0), np.float64(-21.0), np.float64(-20.0), np.float64(-18.0), np.float64(-21.0), np.float64(-21.0)]\n",
      "Last 10 rewards: [np.float64(19.0), np.float64(15.0), np.float64(14.0), np.float64(14.0), np.float64(17.0), np.float64(18.0), np.float64(14.0), np.float64(11.0), np.float64(19.0), np.float64(18.0)]\n",
      "Average loss (first 100): 0.0198\n",
      "Average loss (last 100): 0.0011\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Reward curve for training run\n",
    "# ------------------------------------------------------------\n",
    "# Visualise how the total episode reward changed over time.\n",
    "# This helps us see if the agent is learning to play better.\n",
    "# ============================================================\n",
    "\n",
    "# Create a figure with two subplots: one for rewards, one for loss.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "# ---- Plot 1: Episode Rewards ----\n",
    "# Plot raw episode rewards so we can see the actual performance.\n",
    "ax1.plot(episode_rewards, label=\"Episode reward\", alpha=0.7, color='blue')\n",
    "\n",
    "# Add a simple moving average to smooth the curve and see the trend better.\n",
    "# This helps us see if performance is improving despite the noise.\n",
    "if len(episode_rewards) >= 20:\n",
    "    window = 20\n",
    "    moving_avg = np.convolve(\n",
    "        episode_rewards,\n",
    "        np.ones(window) / window,\n",
    "        mode=\"valid\"\n",
    "    )\n",
    "    ax1.plot(\n",
    "        range(window - 1, len(episode_rewards)),\n",
    "        moving_avg,\n",
    "        label=f\"{window}-episode moving average\",\n",
    "        linewidth=2,\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Total reward\")\n",
    "ax1.set_title(\"Training progress on Pong (Double DQN)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# ---- Plot 2: Training Loss ----\n",
    "# Plot the training loss to see if the network is learning.\n",
    "# Lower loss means the Q-value predictions are getting more accurate.\n",
    "if len(losses) > 0:\n",
    "    # Plot loss over training steps (not episodes).\n",
    "    ax2.plot(losses, alpha=0.7, color='green', label=\"Training loss\")\n",
    "    \n",
    "    # Add a moving average for the loss too.\n",
    "    if len(losses) >= 100:\n",
    "        loss_window = 100\n",
    "        loss_moving_avg = np.convolve(\n",
    "            losses,\n",
    "            np.ones(loss_window) / loss_window,\n",
    "            mode=\"valid\"\n",
    "        )\n",
    "        ax2.plot(\n",
    "            range(loss_window - 1, len(losses)),\n",
    "            loss_moving_avg,\n",
    "            label=f\"{loss_window}-step moving average\",\n",
    "            linewidth=2,\n",
    "            color='orange'\n",
    "        )\n",
    "    \n",
    "    ax2.set_xlabel(\"Training step\")\n",
    "    ax2.set_ylabel(\"Loss (MSE)\")\n",
    "    ax2.set_title(\"Training Loss Over Time\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, \"No loss data available\", \n",
    "             ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title(\"Training Loss Over Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics.\n",
    "print(\"First 10 rewards:\", episode_rewards[:10])\n",
    "print(\"Last 10 rewards:\", episode_rewards[-10:])\n",
    "if len(losses) > 0:\n",
    "    print(f\"Average loss (first 100): {np.mean(losses[:100]):.4f}\")\n",
    "    print(f\"Average loss (last 100): {np.mean(losses[-100:]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a666cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation episode finished. Total reward: -13.0\n",
      "Number of frames collected: 9968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_evaluation_episode(render_first_frame=True):\n",
    "    \"\"\"\n",
    "    Run a single evaluation episode using a purely greedy policy\n",
    "    (no random exploration) over the REDUCED action set.\n",
    "    \n",
    "    This is used to test how well our trained agent performs without\n",
    "    any random exploration - just pure exploitation of what it learned.\n",
    "\n",
    "    Returns the total reward and the list of raw RGB frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put the network in evaluation mode (turns off dropout, batch norm updates, etc.).\n",
    "    policy_net.eval()\n",
    "\n",
    "    # Reset the environment to start a fresh episode.\n",
    "    obs, info = env.reset()\n",
    "    frame_stack = FrameStack(num_frames=4)\n",
    "    state = frame_stack.reset(obs)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    frames_eval = []  # store frames here if we want to turn them into a video later\n",
    "\n",
    "    # Run the episode until it ends.\n",
    "    while not done:\n",
    "        # Convert state to a batch of size 1 so the network can process it.\n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "\n",
    "        # Greedy action selection - always pick the action with highest Q-value.\n",
    "        # No exploration here, we want to see the agent's best performance.\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state_tensor)          # shape: (1, NUM_ACTIONS)\n",
    "            action_index = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        # Convert the action index to the actual Atari action.\n",
    "        env_action = VALID_ACTIONS[action_index]\n",
    "\n",
    "        # Step the environment with our chosen action.\n",
    "        next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update our frame stack to get the next state.\n",
    "        state = frame_stack.step(next_obs)\n",
    "        \n",
    "        # Accumulate the reward (note: we don't clip rewards during evaluation).\n",
    "        total_reward += reward\n",
    "\n",
    "        # Save the raw frame so we can make a video of the agent playing later.\n",
    "        frames_eval.append(next_obs)\n",
    "\n",
    "    # Optionally show the very first frame of the evaluation run.\n",
    "    if render_first_frame and len(frames_eval) > 0:\n",
    "        plt.imshow(frames_eval[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"First frame of evaluation episode\")\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Evaluation episode finished. Total reward: {total_reward:.1f}\")\n",
    "    print(f\"Number of frames collected: {len(frames_eval)}\")\n",
    "\n",
    "    return total_reward, frames_eval\n",
    "\n",
    "\n",
    "# ---- Quick smoke test: run one evaluation episode ----\n",
    "eval_reward, eval_frames = run_evaluation_episode(render_first_frame=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde5d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection episode total reward: -13.0\n",
      "Action counts per ACTION INDEX (mapped via VALID_ACTIONS):\n",
      "  index 0 (env action 0): 2403 steps\n",
      "  index 1 (env action 2): 4124 steps\n",
      "  index 2 (env action 3): 3441 steps\n"
     ]
    }
   ],
   "source": [
    "def inspect_policy_actions():\n",
    "    \"\"\"\n",
    "    Run a single greedy-policy episode and count how many times\n",
    "    each action INDEX (0..NUM_ACTIONS-1) is chosen.\n",
    "\n",
    "    This helps debug collapse to a single action like 'always UP'.\n",
    "    \"\"\"\n",
    "\n",
    "    policy_net.eval()  # make sure we're in eval mode\n",
    "\n",
    "    # Reset environment and frame stack\n",
    "    obs, info = env.reset()\n",
    "    frame_stack = FrameStack(num_frames=4)\n",
    "    state = frame_stack.reset(obs)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    # One counter per possible ACTION INDEX (not raw env action!\n",
    "    action_counts = np.zeros(NUM_ACTIONS, dtype=int)\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "\n",
    "        # Greedy over indices 0..NUM_ACTIONS-1\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action_index = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        env_action = VALID_ACTIONS[action_index]\n",
    "        action_counts[action_index] += 1\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = frame_stack.step(next_obs)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Inspection episode total reward:\", total_reward)\n",
    "    print(\"Action counts per ACTION INDEX (mapped via VALID_ACTIONS):\")\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        print(f\"  index {i} (env action {VALID_ACTIONS[i]}): {action_counts[i]} steps\")\n",
    "\n",
    "# ---- Run the action inspection ---\n",
    "inspect_policy_actions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ccc5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: pong_evaluation.mp4\n"
     ]
    }
   ],
   "source": [
    "def save_video(frames, filename=\"pong_evaluation.mp4\", fps=30):\n",
    "    \"\"\"\n",
    "    Save a list of RGB frames (H, W, 3) as a video file using OpenCV.\n",
    "\n",
    "    - frames: list of numpy arrays from the environment (RGB).\n",
    "    - filename: output video file name.\n",
    "    - fps: frames per second for the video.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        print(\"No frames to save, skipping video.\")\n",
    "        return\n",
    "\n",
    "    # Get frame height and width from the first frame.\n",
    "    height, width, _ = frames[0].shape\n",
    "\n",
    "    # FourCC code for mp4 output; should work on most setups.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        # OpenCV expects BGR, but Gym gives RGB.\n",
    "        bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video_writer.write(bgr_frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to: {filename}\")\n",
    "\n",
    "\n",
    "# Use the frames from the last evaluation run.\n",
    "save_video(eval_frames, filename=\"pong_evaluation.mp4\", fps=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
