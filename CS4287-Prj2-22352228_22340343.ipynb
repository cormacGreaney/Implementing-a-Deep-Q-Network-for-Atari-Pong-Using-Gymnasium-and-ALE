{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cormac Greaney / 22352228 & Jan Lawinski / 22340343\n",
    "# The code does execute to the end without error \n",
    "# Placeholer for links to third party implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d988e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Installation Cell\n",
    "# ------------------------------------------------------------\n",
    "# These pip commands install everything required for this\n",
    "# notebook to run on a fresh machine\n",
    "# ============================================================\n",
    "\n",
    "# --- Core RL + Atari dependencies ---\n",
    "#!pip install \"gymnasium[atari,accept-rom-license]\"\n",
    "#!pip install ale-py\n",
    "\n",
    "# --- Numerical + plotting utilities ---\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "\n",
    "# --- Image processing for preprocessing frames ---\n",
    "#!pip install opencv-python\n",
    "\n",
    "# --- Deep learning framework (we're using PyTorch) ---\n",
    "#!pip install torch\n",
    "\n",
    "# --- (Optional) If you're running on Windows and have issues ---\n",
    "#!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym            # main RL library\n",
    "import numpy as np                 # handy for arrays\n",
    "import matplotlib.pyplot as plt    # for visuals\n",
    "import ale_py                      # Atari emulator backend\n",
    "import cv2                         # image preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Plug the Atari environments into Gymnasium.\n",
    "# Without this, Pong won't appear in the registry.\n",
    "gym.register_envs(ale_py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config initialised. SEED = 42\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Global configuration and random seeds\n",
    "# ------------------------------------------------------------\n",
    "# This makes runs more reproducible and keeps the main\n",
    "# hyperparameters in one place so they are easy to tweak.\n",
    "# ============================================================\n",
    "\n",
    "# Random seeds for reproducibility (not perfectly identical runs,\n",
    "# but it helps a lot to make behaviour more consistent).\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Core DQN / training hyperparameters.\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "REPLAY_BUFFER_CAPACITY = 100_000\n",
    "\n",
    "# Epsilon-greedy schedule.\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END   = 0.1\n",
    "EPSILON_DECAY = 1_000_000  # steps to go from start -> end\n",
    "\n",
    "# Training schedule.\n",
    "NUM_EPISODES = 800          \n",
    "TARGET_UPDATE_INTERVAL = 20\n",
    "MAX_STEPS_PER_EPISODE = 10_000\n",
    "\n",
    "# Double DQN flag: True = Double DQN, False = vanilla DQN target.\n",
    "USE_DOUBLE_DQN = True\n",
    "\n",
    "print(\"Config initialised. SEED =\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b23f5c",
   "metadata": {},
   "source": [
    "# CS4287 Assignment 2 — Deep Reinforcement Learning (Option 2: Atari Pong)\n",
    "# This is the spec he gave for md cells for us to fill in after training and evaluation\n",
    "\n",
    "**Student IDs:** 22352228 (Cormac Greaney), 22340343 (Jan Lawinski)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Reinforcement Learning is appropriate for this task\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Gym Environment: PongNoFrameskip-v4\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Data capture and preprocessing\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "### 3.2 Replay buffer\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "### 3.3 DQN network architecture\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "### 3.4 Q-learning update\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Results\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation of the trained agent\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Independently researched performance improvements\n",
    "\n",
    "*(placeholder.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "*(placeholder.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c725077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs type/shape: <class 'numpy.ndarray'> (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Make a Pong environment that returns RGB frames so we can display them.\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment to start a new game.\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Just print what kind of data we got back.\n",
    "print(\"Obs type/shape:\", type(obs), getattr(obs, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6524f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 200 frames\n"
     ]
    }
   ],
   "source": [
    "# We'll collect a few frames by taking random actions.\n",
    "terminated = False\n",
    "truncated = False\n",
    "frames = []\n",
    "\n",
    "for t in range(200):\n",
    "    # Choose a random action from Pong's action space.\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward.\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Save the frame so we can preview it later.\n",
    "    frames.append(obs)\n",
    "    \n",
    "    # Stop early if the episode ends.\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Collected\", len(frames), \"frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c8a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the very first frame we grabbed.\n",
    "plt.imshow(frames[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d4200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Take a raw (210x160x3) RGB frame from Pong\n",
    "    and convert it into a clean 84x84 grayscale image\n",
    "    that the DQN can actually learn from.\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn the RGB frame into a grayscale image.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Shrink from (210x160) down to (84x84), which is the standard DQN input size.\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert pixel values from 0–255 integers into 0–1 floats.\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Test the preprocessing on the first frame we collected earlier.\n",
    "processed = preprocess_frame(frames[0])\n",
    "\n",
    "print(\"Processed shape:\", processed.shape)\n",
    "\n",
    "# Show the processed grayscale image so we know it looks right.\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16996c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Keep a rolling window of the last N processed frames.\n",
    "    This lets the agent see short-term motion instead of a single static image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_frames=4):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "\n",
    "    def reset(self, initial_frame):\n",
    "        \"\"\"\n",
    "        Called at the start of an episode.\n",
    "        We take the very first raw frame, preprocess it,\n",
    "        and then duplicate it N times so the stacked state is well-defined.\n",
    "        \"\"\"\n",
    "        processed = preprocess_frame(initial_frame)\n",
    "\n",
    "        self.frames.clear()\n",
    "        for _ in range(self.num_frames):\n",
    "            self.frames.append(processed)\n",
    "\n",
    "        # Shape will be (num_frames, 84, 84)\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "    def step(self, new_frame):\n",
    "        \"\"\"\n",
    "        Called every time we get a new raw frame from the environment.\n",
    "        We preprocess it and push it into the stack, automatically\n",
    "        dropping the oldest frame.\n",
    "        \"\"\"\n",
    "        processed = preprocess_frame(new_frame)\n",
    "        self.frames.append(processed)\n",
    "\n",
    "        return np.stack(self.frames, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c2b503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stacked state shape: (4, 84, 84)\n",
      "After step 1, state shape: (4, 84, 84)\n",
      "After step 2, state shape: (4, 84, 84)\n",
      "After step 3, state shape: (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Make a fresh environment reset so we have a clean starting frame.\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Create a frame stacker that holds the last 4 frames.\n",
    "frame_stack = FrameStack(num_frames=4)\n",
    "\n",
    "# Build the initial stacked state from the very first frame.\n",
    "state = frame_stack.reset(obs)\n",
    "print(\"Initial stacked state shape:\", state.shape)  # expected: (4, 84, 84)\n",
    "\n",
    "# Take a few random steps and update the frame stack.\n",
    "for t in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    state = frame_stack.step(obs)\n",
    "    print(f\"After step {t+1}, state shape:\", state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ac7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple experience replay buffer.\n",
    "    Stores (state, action, reward, next_state, done) tuples.\n",
    "\n",
    "    - state / next_state: stacked frames, shape (4, 84, 84)\n",
    "    - action: integer (which action was taken)\n",
    "    - reward: float\n",
    "    - done: bool (True if the episode ended)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # 'capacity' is the max number of transitions we keep.\n",
    "        # Once we hit this limit, the oldest experiences get dropped.\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Save a single experience into the buffer.\n",
    "        We store raw numpy arrays / values here.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        This is what the DQN will train on.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unzip the batch into separate arrays.\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to numpy arrays for easier handling later.\n",
    "        states      = np.stack(states, axis=0)        # (batch, 4, 84, 84)\n",
    "        next_states = np.stack(next_states, axis=0)   # (batch, 4, 84, 84)\n",
    "        actions     = np.array(actions, dtype=np.int64)\n",
    "        rewards     = np.array(rewards, dtype=np.float32)\n",
    "        dones       = np.array(dones, dtype=np.float32)  # will be 0.0 or 1.0\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Let len(buffer) tell us how many experiences we have stored.\n",
    "        This is handy for checking if we have enough to start training.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33701081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Q-values shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Deep Q-Network for Pong.\n",
    "\n",
    "    Input:  (batch, 4, 84, 84) stacked preprocessed frames\n",
    "    Output: (batch, num_actions) predicted Q-values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # First block of convolutions extracts basic motion/edge features.\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "\n",
    "        # Second block extracts mid-level spatial features.\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "\n",
    "        # Third block captures more complex movement patterns.\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # The final size after these conv layers is 64 feature maps of size 7x7.\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "\n",
    "        # Final layer outputs one Q-value per action.\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution layers with ReLU activations.\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten before fully-connected layers.\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Hidden layer + ReLU.\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output layer: raw Q-values.\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Not enough samples in replay buffer yet: 20 / 32\n"
     ]
    }
   ],
   "source": [
    "# Device setup: use GPU if available, otherwise CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Hyperparameters for DQN.\n",
    "gamma = GAMMA          # discount factor for future rewards\n",
    "batch_size = BATCH_SIZE       # how many experiences to sample per training step\n",
    "learning_rate = LEARNING_RATE  # how fast we update the network weights\n",
    "\n",
    "# Create both the policy network and the target network.\n",
    "# - policy_net: the one we train on every step.\n",
    "# - target_net: a slowly updated copy used to compute stable target values.\n",
    "policy_net = DQN(num_actions=env.action_space.n).to(device)\n",
    "target_net = DQN(num_actions=env.action_space.n).to(device)\n",
    "\n",
    "# Copy the weights from policy_net -> target_net at the start.\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # target_net is not trained directly, only updated by copying.\n",
    "\n",
    "# Optimizer: Adam is a solid default choice for DQN.\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def optimize_model(replay_buffer, use_double_dqn=True):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step on the DQN using a batch of\n",
    "    experiences sampled from the replay buffer.\n",
    "\n",
    "    If use_double_dqn is True, we use a Double DQN-style target to\n",
    "    reduce maximisation bias:\n",
    "\n",
    "        a* = argmax_a Q_policy(s', a)\n",
    "        y  = r + gamma * Q_target(s', a*)\n",
    "\n",
    "    Otherwise we fall back to the standard DQN target:\n",
    "\n",
    "        y = r + gamma * max_a Q_target(s', a)\n",
    "    \"\"\"\n",
    "\n",
    "    # We need at least 'batch_size' experiences before we can train.\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None  # not enough data yet, skip the update\n",
    "\n",
    "    # ---- 1. Sample a batch of experiences from the replay buffer ----\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and move them to the right device.\n",
    "    states      = torch.from_numpy(states).to(device)          # (batch, 4, 84, 84)\n",
    "    next_states = torch.from_numpy(next_states).to(device)     # (batch, 4, 84, 84)\n",
    "    actions     = torch.from_numpy(actions).to(device)         # (batch,)\n",
    "    rewards     = torch.from_numpy(rewards).to(device)         # (batch,)\n",
    "    dones       = torch.from_numpy(dones).to(device)           # (batch,)\n",
    "\n",
    "    # ---- 2. Compute Q(s, a) for the actions actually taken ----\n",
    "    # policy_net(states) gives Q-values for ALL actions in each state: shape (batch, num_actions).\n",
    "    q_values = policy_net(states)  # Q(s, ·)\n",
    "\n",
    "    # We only want the Q-values for the specific actions we took.\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Q(s, a_taken)\n",
    "\n",
    "    # ---- 3. Compute target values ----\n",
    "    with torch.no_grad():\n",
    "        if use_double_dqn:\n",
    "            # -------- Double DQN target --------\n",
    "            # 1) Use the *policy* network to pick the best next action.\n",
    "            next_q_policy = policy_net(next_states)                 # Q_policy(s', ·)\n",
    "            best_next_actions = next_q_policy.argmax(dim=1)         # a* for each sample\n",
    "\n",
    "            # 2) Use the *target* network to evaluate those actions.\n",
    "            next_q_target = target_net(next_states)                 # Q_target(s', ·)\n",
    "            max_next_q_values = next_q_target.gather(\n",
    "                1, best_next_actions.unsqueeze(1)\n",
    "            ).squeeze(1)                                           # Q_target(s', a*)\n",
    "        else:\n",
    "            # -------- Standard DQN target --------\n",
    "            next_q_values = target_net(next_states)                 # Q_target(s', ·)\n",
    "            max_next_q_values, _ = next_q_values.max(dim=1)        # max_a Q_target(s', a)\n",
    "\n",
    "        # If done == 1, we don't bootstrap from the next state (no future reward).\n",
    "        target_values = rewards + gamma * max_next_q_values * (1.0 - dones)\n",
    "\n",
    "    # ---- 4. Compute the loss between current Q(s, a) and target values ----\n",
    "    loss = F.mse_loss(state_action_values, target_values)\n",
    "\n",
    "    # ---- 5. Backpropagation: update the policy network weights ----\n",
    "    optimizer.zero_grad()  # clear old gradients\n",
    "    loss.backward()        # compute new gradients\n",
    "    optimizer.step()       # update the weights\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0794069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, epsilon=1.000, chosen action=1\n",
      "Step 1, epsilon=1.000, chosen action=3\n",
      "Step 2, epsilon=1.000, chosen action=5\n",
      "Step 3, epsilon=1.000, chosen action=3\n",
      "Step 4, epsilon=1.000, chosen action=5\n"
     ]
    }
   ],
   "source": [
    "# This will track how many environment steps we've taken so far.\n",
    "# We'll use it to slowly decay epsilon over time.\n",
    "steps_done = 0\n",
    "\n",
    "def get_epsilon(step):\n",
    "    \"\"\"\n",
    "    Linearly decay epsilon from epsilon_start down to epsilon_end\n",
    "    over 'epsilon_decay' steps.\n",
    "\n",
    "    After that many steps, epsilon stays at epsilon_end.\n",
    "    \"\"\"\n",
    "    if step >= EPSILON_DECAY:\n",
    "        return EPSILON_END\n",
    "    \n",
    "    # Linear interpolation between start and end.\n",
    "    fraction = step / EPSILON_DECAY\n",
    "    return EPSILON_START + fraction * (EPSILON_END - EPSILON_START)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Choose an action using an epsilon-greedy policy.\n",
    "\n",
    "    - With probability epsilon: pick a random action.\n",
    "    - Otherwise: pick the action with the highest Q-value from the policy network.\n",
    "\n",
    "    'state' is expected to be a numpy array with shape (4, 84, 84).\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "\n",
    "    # Compute current epsilon based on how many steps we've taken.\n",
    "    epsilon = get_epsilon(steps_done)\n",
    "    steps_done += 1\n",
    "\n",
    "    # Decide: explore or exploit?\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: random action.\n",
    "        action = env.action_space.sample()\n",
    "        return action, epsilon\n",
    "\n",
    "    # Exploit: choose best action according to the current Q-network.\n",
    "    # We need to turn state into a torch tensor with shape (1, 4, 84, 84).\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)  # add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state_tensor)  # shape: (1, num_actions)\n",
    "        # Take the index of the largest Q-value.\n",
    "        action = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    return action, epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13483b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2025-11-13 01:55:34.423567\n",
      "Starting training...\n",
      "\n",
      "Episode    1/500 | Reward: -20.0 | Epsilon: 0.996 | Buffer: 10000\n",
      "Episode    2/500 | Reward: -21.0 | Epsilon: 0.993 | Buffer: 10000\n",
      "Episode    3/500 | Reward: -20.0 | Epsilon: 0.990 | Buffer: 10000\n",
      "Episode    4/500 | Reward: -21.0 | Epsilon: 0.987 | Buffer: 10000\n",
      "Episode    5/500 | Reward: -21.0 | Epsilon: 0.984 | Buffer: 10000\n",
      "Episode    6/500 | Reward: -21.0 | Epsilon: 0.982 | Buffer: 10000\n",
      "Episode    7/500 | Reward: -21.0 | Epsilon: 0.979 | Buffer: 10000\n",
      "Episode    8/500 | Reward: -21.0 | Epsilon: 0.976 | Buffer: 10000\n",
      "Episode    9/500 | Reward: -21.0 | Epsilon: 0.973 | Buffer: 10000\n",
      "Episode   10/500 | Reward: -21.0 | Epsilon: 0.969 | Buffer: 10000\n",
      "Episode   11/500 | Reward: -21.0 | Epsilon: 0.967 | Buffer: 10000\n",
      "Episode   12/500 | Reward: -21.0 | Epsilon: 0.963 | Buffer: 10000\n",
      "Episode   13/500 | Reward: -21.0 | Epsilon: 0.960 | Buffer: 10000\n",
      "Episode   14/500 | Reward: -20.0 | Epsilon: 0.957 | Buffer: 10000\n",
      "Episode   15/500 | Reward: -21.0 | Epsilon: 0.954 | Buffer: 10000\n",
      "Episode   16/500 | Reward: -21.0 | Epsilon: 0.952 | Buffer: 10000\n",
      "Episode   17/500 | Reward: -21.0 | Epsilon: 0.948 | Buffer: 10000\n",
      "Episode   18/500 | Reward: -20.0 | Epsilon: 0.945 | Buffer: 10000\n",
      "Episode   19/500 | Reward: -20.0 | Epsilon: 0.941 | Buffer: 10000\n",
      "Episode   20/500 | Reward: -21.0 | Epsilon: 0.938 | Buffer: 10000\n",
      "Episode   21/500 | Reward: -21.0 | Epsilon: 0.935 | Buffer: 10000\n",
      "Episode   22/500 | Reward: -21.0 | Epsilon: 0.932 | Buffer: 10000\n",
      "Episode   23/500 | Reward: -21.0 | Epsilon: 0.929 | Buffer: 10000\n",
      "Episode   24/500 | Reward: -21.0 | Epsilon: 0.926 | Buffer: 10000\n",
      "Episode   25/500 | Reward: -20.0 | Epsilon: 0.923 | Buffer: 10000\n",
      "Episode   26/500 | Reward: -21.0 | Epsilon: 0.920 | Buffer: 10000\n",
      "Episode   27/500 | Reward: -20.0 | Epsilon: 0.917 | Buffer: 10000\n",
      "Episode   28/500 | Reward: -20.0 | Epsilon: 0.913 | Buffer: 10000\n",
      "Episode   29/500 | Reward: -21.0 | Epsilon: 0.910 | Buffer: 10000\n",
      "Episode   30/500 | Reward: -21.0 | Epsilon: 0.907 | Buffer: 10000\n",
      "Episode   31/500 | Reward: -20.0 | Epsilon: 0.904 | Buffer: 10000\n",
      "Episode   32/500 | Reward: -21.0 | Epsilon: 0.901 | Buffer: 10000\n",
      "Episode   33/500 | Reward: -21.0 | Epsilon: 0.897 | Buffer: 10000\n",
      "Episode   34/500 | Reward: -20.0 | Epsilon: 0.894 | Buffer: 10000\n",
      "Episode   35/500 | Reward: -21.0 | Epsilon: 0.891 | Buffer: 10000\n",
      "Episode   36/500 | Reward: -20.0 | Epsilon: 0.888 | Buffer: 10000\n",
      "Episode   37/500 | Reward: -21.0 | Epsilon: 0.885 | Buffer: 10000\n",
      "Episode   38/500 | Reward: -21.0 | Epsilon: 0.882 | Buffer: 10000\n",
      "Episode   39/500 | Reward: -20.0 | Epsilon: 0.879 | Buffer: 10000\n",
      "Episode   40/500 | Reward: -21.0 | Epsilon: 0.876 | Buffer: 10000\n",
      "Episode   41/500 | Reward: -21.0 | Epsilon: 0.873 | Buffer: 10000\n",
      "Episode   42/500 | Reward: -20.0 | Epsilon: 0.870 | Buffer: 10000\n",
      "Episode   43/500 | Reward: -21.0 | Epsilon: 0.867 | Buffer: 10000\n",
      "Episode   44/500 | Reward: -21.0 | Epsilon: 0.864 | Buffer: 10000\n",
      "Episode   45/500 | Reward: -18.0 | Epsilon: 0.861 | Buffer: 10000\n",
      "Episode   46/500 | Reward: -21.0 | Epsilon: 0.858 | Buffer: 10000\n",
      "Episode   47/500 | Reward: -21.0 | Epsilon: 0.855 | Buffer: 10000\n",
      "Episode   48/500 | Reward: -21.0 | Epsilon: 0.852 | Buffer: 10000\n",
      "Episode   49/500 | Reward: -21.0 | Epsilon: 0.849 | Buffer: 10000\n",
      "Episode   50/500 | Reward: -21.0 | Epsilon: 0.846 | Buffer: 10000\n",
      "Episode   51/500 | Reward: -20.0 | Epsilon: 0.842 | Buffer: 10000\n",
      "Episode   52/500 | Reward: -19.0 | Epsilon: 0.838 | Buffer: 10000\n",
      "Episode   53/500 | Reward: -20.0 | Epsilon: 0.835 | Buffer: 10000\n",
      "Episode   54/500 | Reward: -21.0 | Epsilon: 0.832 | Buffer: 10000\n",
      "Episode   55/500 | Reward: -19.0 | Epsilon: 0.828 | Buffer: 10000\n",
      "Episode   56/500 | Reward: -20.0 | Epsilon: 0.825 | Buffer: 10000\n",
      "Episode   57/500 | Reward: -20.0 | Epsilon: 0.822 | Buffer: 10000\n",
      "Episode   58/500 | Reward: -21.0 | Epsilon: 0.819 | Buffer: 10000\n",
      "Episode   59/500 | Reward: -21.0 | Epsilon: 0.815 | Buffer: 10000\n",
      "Episode   60/500 | Reward: -21.0 | Epsilon: 0.812 | Buffer: 10000\n",
      "Episode   61/500 | Reward: -20.0 | Epsilon: 0.809 | Buffer: 10000\n",
      "Episode   62/500 | Reward: -20.0 | Epsilon: 0.806 | Buffer: 10000\n",
      "Episode   63/500 | Reward: -20.0 | Epsilon: 0.803 | Buffer: 10000\n",
      "Episode   64/500 | Reward: -21.0 | Epsilon: 0.800 | Buffer: 10000\n",
      "Episode   65/500 | Reward: -20.0 | Epsilon: 0.797 | Buffer: 10000\n",
      "Episode   66/500 | Reward: -20.0 | Epsilon: 0.794 | Buffer: 10000\n",
      "Episode   67/500 | Reward: -19.0 | Epsilon: 0.790 | Buffer: 10000\n",
      "Episode   68/500 | Reward: -19.0 | Epsilon: 0.787 | Buffer: 10000\n",
      "Episode   69/500 | Reward: -21.0 | Epsilon: 0.784 | Buffer: 10000\n",
      "Episode   70/500 | Reward: -21.0 | Epsilon: 0.780 | Buffer: 10000\n",
      "Episode   71/500 | Reward: -19.0 | Epsilon: 0.777 | Buffer: 10000\n",
      "Episode   72/500 | Reward: -21.0 | Epsilon: 0.774 | Buffer: 10000\n",
      "Episode   73/500 | Reward: -20.0 | Epsilon: 0.771 | Buffer: 10000\n",
      "Episode   74/500 | Reward: -20.0 | Epsilon: 0.768 | Buffer: 10000\n",
      "Episode   75/500 | Reward: -21.0 | Epsilon: 0.765 | Buffer: 10000\n",
      "Episode   76/500 | Reward: -21.0 | Epsilon: 0.762 | Buffer: 10000\n",
      "Episode   77/500 | Reward: -21.0 | Epsilon: 0.759 | Buffer: 10000\n",
      "Episode   78/500 | Reward: -20.0 | Epsilon: 0.756 | Buffer: 10000\n",
      "Episode   79/500 | Reward: -20.0 | Epsilon: 0.753 | Buffer: 10000\n",
      "Episode   80/500 | Reward: -19.0 | Epsilon: 0.749 | Buffer: 10000\n",
      "Episode   81/500 | Reward: -21.0 | Epsilon: 0.746 | Buffer: 10000\n",
      "Episode   82/500 | Reward: -21.0 | Epsilon: 0.743 | Buffer: 10000\n",
      "Episode   83/500 | Reward: -20.0 | Epsilon: 0.740 | Buffer: 10000\n",
      "Episode   84/500 | Reward: -21.0 | Epsilon: 0.737 | Buffer: 10000\n",
      "Episode   85/500 | Reward: -20.0 | Epsilon: 0.733 | Buffer: 10000\n",
      "Episode   86/500 | Reward: -21.0 | Epsilon: 0.731 | Buffer: 10000\n",
      "Episode   87/500 | Reward: -21.0 | Epsilon: 0.728 | Buffer: 10000\n",
      "Episode   88/500 | Reward: -21.0 | Epsilon: 0.725 | Buffer: 10000\n",
      "Episode   89/500 | Reward: -21.0 | Epsilon: 0.722 | Buffer: 10000\n",
      "Episode   90/500 | Reward: -20.0 | Epsilon: 0.719 | Buffer: 10000\n",
      "Episode   91/500 | Reward: -20.0 | Epsilon: 0.716 | Buffer: 10000\n",
      "Episode   92/500 | Reward: -20.0 | Epsilon: 0.712 | Buffer: 10000\n",
      "Episode   93/500 | Reward: -21.0 | Epsilon: 0.709 | Buffer: 10000\n",
      "Episode   94/500 | Reward: -20.0 | Epsilon: 0.706 | Buffer: 10000\n",
      "Episode   95/500 | Reward: -21.0 | Epsilon: 0.703 | Buffer: 10000\n",
      "Episode   96/500 | Reward: -20.0 | Epsilon: 0.700 | Buffer: 10000\n",
      "Episode   97/500 | Reward: -21.0 | Epsilon: 0.697 | Buffer: 10000\n",
      "Episode   98/500 | Reward: -21.0 | Epsilon: 0.694 | Buffer: 10000\n",
      "Episode   99/500 | Reward: -21.0 | Epsilon: 0.691 | Buffer: 10000\n",
      "Episode  100/500 | Reward: -21.0 | Epsilon: 0.688 | Buffer: 10000\n",
      "Episode  101/500 | Reward: -21.0 | Epsilon: 0.684 | Buffer: 10000\n",
      "Episode  102/500 | Reward: -21.0 | Epsilon: 0.681 | Buffer: 10000\n",
      "Episode  103/500 | Reward: -21.0 | Epsilon: 0.679 | Buffer: 10000\n",
      "Episode  104/500 | Reward: -21.0 | Epsilon: 0.675 | Buffer: 10000\n",
      "Episode  105/500 | Reward: -21.0 | Epsilon: 0.672 | Buffer: 10000\n",
      "Episode  106/500 | Reward: -21.0 | Epsilon: 0.669 | Buffer: 10000\n",
      "Episode  107/500 | Reward: -21.0 | Epsilon: 0.667 | Buffer: 10000\n",
      "Episode  108/500 | Reward: -21.0 | Epsilon: 0.664 | Buffer: 10000\n",
      "Episode  109/500 | Reward: -21.0 | Epsilon: 0.661 | Buffer: 10000\n",
      "Episode  110/500 | Reward: -21.0 | Epsilon: 0.658 | Buffer: 10000\n",
      "Episode  111/500 | Reward: -21.0 | Epsilon: 0.655 | Buffer: 10000\n",
      "Episode  112/500 | Reward: -21.0 | Epsilon: 0.652 | Buffer: 10000\n",
      "Episode  113/500 | Reward: -21.0 | Epsilon: 0.649 | Buffer: 10000\n",
      "Episode  114/500 | Reward: -20.0 | Epsilon: 0.646 | Buffer: 10000\n",
      "Episode  115/500 | Reward: -21.0 | Epsilon: 0.643 | Buffer: 10000\n",
      "Episode  116/500 | Reward: -21.0 | Epsilon: 0.640 | Buffer: 10000\n",
      "Episode  117/500 | Reward: -21.0 | Epsilon: 0.637 | Buffer: 10000\n",
      "Episode  118/500 | Reward: -20.0 | Epsilon: 0.634 | Buffer: 10000\n",
      "Episode  119/500 | Reward: -21.0 | Epsilon: 0.631 | Buffer: 10000\n",
      "Episode  120/500 | Reward: -20.0 | Epsilon: 0.627 | Buffer: 10000\n",
      "Episode  121/500 | Reward: -21.0 | Epsilon: 0.624 | Buffer: 10000\n",
      "Episode  122/500 | Reward: -21.0 | Epsilon: 0.622 | Buffer: 10000\n",
      "Episode  123/500 | Reward: -21.0 | Epsilon: 0.619 | Buffer: 10000\n",
      "Episode  124/500 | Reward: -21.0 | Epsilon: 0.616 | Buffer: 10000\n",
      "Episode  125/500 | Reward: -21.0 | Epsilon: 0.613 | Buffer: 10000\n",
      "Episode  126/500 | Reward: -21.0 | Epsilon: 0.610 | Buffer: 10000\n",
      "Episode  127/500 | Reward: -21.0 | Epsilon: 0.607 | Buffer: 10000\n",
      "Episode  128/500 | Reward: -21.0 | Epsilon: 0.605 | Buffer: 10000\n",
      "Episode  129/500 | Reward: -20.0 | Epsilon: 0.602 | Buffer: 10000\n",
      "Episode  130/500 | Reward: -20.0 | Epsilon: 0.598 | Buffer: 10000\n",
      "Episode  131/500 | Reward: -21.0 | Epsilon: 0.596 | Buffer: 10000\n",
      "Episode  132/500 | Reward: -21.0 | Epsilon: 0.593 | Buffer: 10000\n",
      "Episode  133/500 | Reward: -21.0 | Epsilon: 0.590 | Buffer: 10000\n",
      "Episode  134/500 | Reward: -21.0 | Epsilon: 0.587 | Buffer: 10000\n",
      "Episode  135/500 | Reward: -19.0 | Epsilon: 0.583 | Buffer: 10000\n",
      "Episode  136/500 | Reward: -21.0 | Epsilon: 0.580 | Buffer: 10000\n",
      "Episode  137/500 | Reward: -20.0 | Epsilon: 0.577 | Buffer: 10000\n",
      "Episode  138/500 | Reward: -21.0 | Epsilon: 0.574 | Buffer: 10000\n",
      "Episode  139/500 | Reward: -21.0 | Epsilon: 0.571 | Buffer: 10000\n",
      "Episode  140/500 | Reward: -21.0 | Epsilon: 0.568 | Buffer: 10000\n",
      "Episode  141/500 | Reward: -20.0 | Epsilon: 0.565 | Buffer: 10000\n",
      "Episode  142/500 | Reward: -21.0 | Epsilon: 0.562 | Buffer: 10000\n",
      "Episode  143/500 | Reward: -21.0 | Epsilon: 0.560 | Buffer: 10000\n",
      "Episode  144/500 | Reward: -21.0 | Epsilon: 0.557 | Buffer: 10000\n",
      "Episode  145/500 | Reward: -21.0 | Epsilon: 0.554 | Buffer: 10000\n",
      "Episode  146/500 | Reward: -21.0 | Epsilon: 0.551 | Buffer: 10000\n",
      "Episode  147/500 | Reward: -21.0 | Epsilon: 0.548 | Buffer: 10000\n",
      "Episode  148/500 | Reward: -19.0 | Epsilon: 0.545 | Buffer: 10000\n",
      "Episode  149/500 | Reward: -21.0 | Epsilon: 0.542 | Buffer: 10000\n",
      "Episode  150/500 | Reward: -21.0 | Epsilon: 0.539 | Buffer: 10000\n",
      "Episode  151/500 | Reward: -21.0 | Epsilon: 0.536 | Buffer: 10000\n",
      "Episode  152/500 | Reward: -21.0 | Epsilon: 0.533 | Buffer: 10000\n",
      "Episode  153/500 | Reward: -21.0 | Epsilon: 0.530 | Buffer: 10000\n",
      "Episode  154/500 | Reward: -21.0 | Epsilon: 0.527 | Buffer: 10000\n",
      "Episode  155/500 | Reward: -21.0 | Epsilon: 0.524 | Buffer: 10000\n",
      "Episode  156/500 | Reward: -21.0 | Epsilon: 0.521 | Buffer: 10000\n",
      "Episode  157/500 | Reward: -21.0 | Epsilon: 0.518 | Buffer: 10000\n",
      "Episode  158/500 | Reward: -21.0 | Epsilon: 0.515 | Buffer: 10000\n",
      "Episode  159/500 | Reward: -20.0 | Epsilon: 0.512 | Buffer: 10000\n",
      "Episode  160/500 | Reward: -21.0 | Epsilon: 0.509 | Buffer: 10000\n",
      "Episode  161/500 | Reward: -21.0 | Epsilon: 0.507 | Buffer: 10000\n",
      "Episode  162/500 | Reward: -21.0 | Epsilon: 0.504 | Buffer: 10000\n",
      "Episode  163/500 | Reward: -21.0 | Epsilon: 0.501 | Buffer: 10000\n",
      "Episode  164/500 | Reward: -20.0 | Epsilon: 0.497 | Buffer: 10000\n",
      "Episode  165/500 | Reward: -21.0 | Epsilon: 0.494 | Buffer: 10000\n",
      "Episode  166/500 | Reward: -21.0 | Epsilon: 0.492 | Buffer: 10000\n",
      "Episode  167/500 | Reward: -21.0 | Epsilon: 0.489 | Buffer: 10000\n",
      "Episode  168/500 | Reward: -21.0 | Epsilon: 0.486 | Buffer: 10000\n",
      "Episode  169/500 | Reward: -21.0 | Epsilon: 0.483 | Buffer: 10000\n",
      "Episode  170/500 | Reward: -20.0 | Epsilon: 0.480 | Buffer: 10000\n",
      "Episode  171/500 | Reward: -21.0 | Epsilon: 0.477 | Buffer: 10000\n",
      "Episode  172/500 | Reward: -21.0 | Epsilon: 0.474 | Buffer: 10000\n",
      "Episode  173/500 | Reward: -21.0 | Epsilon: 0.471 | Buffer: 10000\n",
      "Episode  174/500 | Reward: -21.0 | Epsilon: 0.469 | Buffer: 10000\n",
      "Episode  175/500 | Reward: -21.0 | Epsilon: 0.466 | Buffer: 10000\n",
      "Episode  176/500 | Reward: -20.0 | Epsilon: 0.463 | Buffer: 10000\n",
      "Episode  177/500 | Reward: -21.0 | Epsilon: 0.460 | Buffer: 10000\n",
      "Episode  178/500 | Reward: -20.0 | Epsilon: 0.457 | Buffer: 10000\n",
      "Episode  179/500 | Reward: -21.0 | Epsilon: 0.454 | Buffer: 10000\n",
      "Episode  180/500 | Reward: -21.0 | Epsilon: 0.451 | Buffer: 10000\n",
      "Episode  181/500 | Reward: -21.0 | Epsilon: 0.448 | Buffer: 10000\n",
      "Episode  182/500 | Reward: -21.0 | Epsilon: 0.445 | Buffer: 10000\n",
      "Episode  183/500 | Reward: -21.0 | Epsilon: 0.443 | Buffer: 10000\n",
      "Episode  184/500 | Reward: -21.0 | Epsilon: 0.440 | Buffer: 10000\n",
      "Episode  185/500 | Reward: -21.0 | Epsilon: 0.437 | Buffer: 10000\n",
      "Episode  186/500 | Reward: -21.0 | Epsilon: 0.434 | Buffer: 10000\n",
      "Episode  187/500 | Reward: -21.0 | Epsilon: 0.431 | Buffer: 10000\n",
      "Episode  188/500 | Reward: -20.0 | Epsilon: 0.428 | Buffer: 10000\n",
      "Episode  189/500 | Reward: -21.0 | Epsilon: 0.425 | Buffer: 10000\n",
      "Episode  190/500 | Reward: -21.0 | Epsilon: 0.422 | Buffer: 10000\n",
      "Episode  191/500 | Reward: -20.0 | Epsilon: 0.419 | Buffer: 10000\n",
      "Episode  192/500 | Reward: -21.0 | Epsilon: 0.416 | Buffer: 10000\n",
      "Episode  193/500 | Reward: -21.0 | Epsilon: 0.414 | Buffer: 10000\n",
      "Episode  194/500 | Reward: -20.0 | Epsilon: 0.411 | Buffer: 10000\n",
      "Episode  195/500 | Reward: -20.0 | Epsilon: 0.407 | Buffer: 10000\n",
      "Episode  196/500 | Reward: -21.0 | Epsilon: 0.405 | Buffer: 10000\n",
      "Episode  197/500 | Reward: -21.0 | Epsilon: 0.402 | Buffer: 10000\n",
      "Episode  198/500 | Reward: -21.0 | Epsilon: 0.399 | Buffer: 10000\n",
      "Episode  199/500 | Reward: -21.0 | Epsilon: 0.396 | Buffer: 10000\n",
      "Episode  200/500 | Reward: -21.0 | Epsilon: 0.393 | Buffer: 10000\n",
      "Episode  201/500 | Reward: -21.0 | Epsilon: 0.390 | Buffer: 10000\n",
      "Episode  202/500 | Reward: -20.0 | Epsilon: 0.387 | Buffer: 10000\n",
      "Episode  203/500 | Reward: -21.0 | Epsilon: 0.384 | Buffer: 10000\n",
      "Episode  204/500 | Reward: -21.0 | Epsilon: 0.382 | Buffer: 10000\n",
      "Episode  205/500 | Reward: -21.0 | Epsilon: 0.379 | Buffer: 10000\n",
      "Episode  206/500 | Reward: -21.0 | Epsilon: 0.376 | Buffer: 10000\n",
      "Episode  207/500 | Reward: -20.0 | Epsilon: 0.372 | Buffer: 10000\n",
      "Episode  208/500 | Reward: -21.0 | Epsilon: 0.370 | Buffer: 10000\n",
      "Episode  209/500 | Reward: -20.0 | Epsilon: 0.367 | Buffer: 10000\n",
      "Episode  210/500 | Reward: -21.0 | Epsilon: 0.364 | Buffer: 10000\n",
      "Episode  211/500 | Reward: -21.0 | Epsilon: 0.361 | Buffer: 10000\n",
      "Episode  212/500 | Reward: -20.0 | Epsilon: 0.358 | Buffer: 10000\n",
      "Episode  213/500 | Reward: -21.0 | Epsilon: 0.355 | Buffer: 10000\n",
      "Episode  214/500 | Reward: -19.0 | Epsilon: 0.351 | Buffer: 10000\n",
      "Episode  215/500 | Reward: -21.0 | Epsilon: 0.348 | Buffer: 10000\n",
      "Episode  216/500 | Reward: -21.0 | Epsilon: 0.345 | Buffer: 10000\n",
      "Episode  217/500 | Reward: -21.0 | Epsilon: 0.342 | Buffer: 10000\n",
      "Episode  218/500 | Reward: -21.0 | Epsilon: 0.340 | Buffer: 10000\n",
      "Episode  219/500 | Reward: -21.0 | Epsilon: 0.337 | Buffer: 10000\n",
      "Episode  220/500 | Reward: -21.0 | Epsilon: 0.334 | Buffer: 10000\n",
      "Episode  221/500 | Reward: -21.0 | Epsilon: 0.331 | Buffer: 10000\n",
      "Episode  222/500 | Reward: -21.0 | Epsilon: 0.328 | Buffer: 10000\n",
      "Episode  223/500 | Reward: -20.0 | Epsilon: 0.325 | Buffer: 10000\n",
      "Episode  224/500 | Reward: -20.0 | Epsilon: 0.322 | Buffer: 10000\n",
      "Episode  225/500 | Reward: -21.0 | Epsilon: 0.319 | Buffer: 10000\n",
      "Episode  226/500 | Reward: -21.0 | Epsilon: 0.316 | Buffer: 10000\n",
      "Episode  227/500 | Reward: -21.0 | Epsilon: 0.313 | Buffer: 10000\n",
      "Episode  228/500 | Reward: -21.0 | Epsilon: 0.310 | Buffer: 10000\n",
      "Episode  229/500 | Reward: -20.0 | Epsilon: 0.307 | Buffer: 10000\n",
      "Episode  230/500 | Reward: -21.0 | Epsilon: 0.304 | Buffer: 10000\n",
      "Episode  231/500 | Reward: -21.0 | Epsilon: 0.301 | Buffer: 10000\n",
      "Episode  232/500 | Reward: -20.0 | Epsilon: 0.298 | Buffer: 10000\n",
      "Episode  233/500 | Reward: -21.0 | Epsilon: 0.295 | Buffer: 10000\n",
      "Episode  234/500 | Reward: -21.0 | Epsilon: 0.292 | Buffer: 10000\n",
      "Episode  235/500 | Reward: -20.0 | Epsilon: 0.289 | Buffer: 10000\n",
      "Episode  236/500 | Reward: -21.0 | Epsilon: 0.287 | Buffer: 10000\n",
      "Episode  237/500 | Reward: -21.0 | Epsilon: 0.284 | Buffer: 10000\n",
      "Episode  238/500 | Reward: -20.0 | Epsilon: 0.281 | Buffer: 10000\n",
      "Episode  239/500 | Reward: -20.0 | Epsilon: 0.278 | Buffer: 10000\n",
      "Episode  240/500 | Reward: -21.0 | Epsilon: 0.275 | Buffer: 10000\n",
      "Episode  241/500 | Reward: -20.0 | Epsilon: 0.272 | Buffer: 10000\n",
      "Episode  242/500 | Reward: -20.0 | Epsilon: 0.269 | Buffer: 10000\n",
      "Episode  243/500 | Reward: -20.0 | Epsilon: 0.266 | Buffer: 10000\n",
      "Episode  244/500 | Reward: -21.0 | Epsilon: 0.262 | Buffer: 10000\n",
      "Episode  245/500 | Reward: -21.0 | Epsilon: 0.260 | Buffer: 10000\n",
      "Episode  246/500 | Reward: -21.0 | Epsilon: 0.257 | Buffer: 10000\n",
      "Episode  247/500 | Reward: -20.0 | Epsilon: 0.254 | Buffer: 10000\n",
      "Episode  248/500 | Reward: -21.0 | Epsilon: 0.251 | Buffer: 10000\n",
      "Episode  249/500 | Reward: -21.0 | Epsilon: 0.248 | Buffer: 10000\n",
      "Episode  250/500 | Reward: -20.0 | Epsilon: 0.245 | Buffer: 10000\n",
      "Episode  251/500 | Reward: -20.0 | Epsilon: 0.242 | Buffer: 10000\n",
      "Episode  252/500 | Reward: -21.0 | Epsilon: 0.239 | Buffer: 10000\n",
      "Episode  253/500 | Reward: -21.0 | Epsilon: 0.236 | Buffer: 10000\n",
      "Episode  254/500 | Reward: -21.0 | Epsilon: 0.233 | Buffer: 10000\n",
      "Episode  255/500 | Reward: -21.0 | Epsilon: 0.230 | Buffer: 10000\n",
      "Episode  256/500 | Reward: -21.0 | Epsilon: 0.227 | Buffer: 10000\n",
      "Episode  257/500 | Reward: -21.0 | Epsilon: 0.224 | Buffer: 10000\n",
      "Episode  258/500 | Reward: -21.0 | Epsilon: 0.222 | Buffer: 10000\n",
      "Episode  259/500 | Reward: -21.0 | Epsilon: 0.219 | Buffer: 10000\n",
      "Episode  260/500 | Reward: -21.0 | Epsilon: 0.216 | Buffer: 10000\n",
      "Episode  261/500 | Reward: -21.0 | Epsilon: 0.213 | Buffer: 10000\n",
      "Episode  262/500 | Reward: -20.0 | Epsilon: 0.210 | Buffer: 10000\n",
      "Episode  263/500 | Reward: -21.0 | Epsilon: 0.207 | Buffer: 10000\n",
      "Episode  264/500 | Reward: -21.0 | Epsilon: 0.204 | Buffer: 10000\n",
      "Episode  265/500 | Reward: -21.0 | Epsilon: 0.201 | Buffer: 10000\n",
      "Episode  266/500 | Reward: -20.0 | Epsilon: 0.198 | Buffer: 10000\n",
      "Episode  267/500 | Reward: -20.0 | Epsilon: 0.195 | Buffer: 10000\n",
      "Episode  268/500 | Reward: -20.0 | Epsilon: 0.192 | Buffer: 10000\n",
      "Episode  269/500 | Reward: -21.0 | Epsilon: 0.189 | Buffer: 10000\n",
      "Episode  270/500 | Reward: -21.0 | Epsilon: 0.186 | Buffer: 10000\n",
      "Episode  271/500 | Reward: -20.0 | Epsilon: 0.183 | Buffer: 10000\n",
      "Episode  272/500 | Reward: -20.0 | Epsilon: 0.180 | Buffer: 10000\n",
      "Episode  273/500 | Reward: -20.0 | Epsilon: 0.177 | Buffer: 10000\n",
      "Episode  274/500 | Reward: -21.0 | Epsilon: 0.174 | Buffer: 10000\n",
      "Episode  275/500 | Reward: -21.0 | Epsilon: 0.171 | Buffer: 10000\n",
      "Episode  276/500 | Reward: -21.0 | Epsilon: 0.168 | Buffer: 10000\n",
      "Episode  277/500 | Reward: -21.0 | Epsilon: 0.164 | Buffer: 10000\n",
      "Episode  278/500 | Reward: -21.0 | Epsilon: 0.161 | Buffer: 10000\n",
      "Episode  279/500 | Reward: -21.0 | Epsilon: 0.158 | Buffer: 10000\n",
      "Episode  280/500 | Reward: -20.0 | Epsilon: 0.155 | Buffer: 10000\n",
      "Episode  281/500 | Reward: -20.0 | Epsilon: 0.152 | Buffer: 10000\n",
      "Episode  282/500 | Reward: -21.0 | Epsilon: 0.149 | Buffer: 10000\n",
      "Episode  283/500 | Reward: -20.0 | Epsilon: 0.146 | Buffer: 10000\n",
      "Episode  284/500 | Reward: -21.0 | Epsilon: 0.143 | Buffer: 10000\n",
      "Episode  285/500 | Reward: -21.0 | Epsilon: 0.140 | Buffer: 10000\n",
      "Episode  286/500 | Reward: -21.0 | Epsilon: 0.137 | Buffer: 10000\n",
      "Episode  287/500 | Reward: -21.0 | Epsilon: 0.133 | Buffer: 10000\n",
      "Episode  288/500 | Reward: -21.0 | Epsilon: 0.131 | Buffer: 10000\n",
      "Episode  289/500 | Reward: -21.0 | Epsilon: 0.128 | Buffer: 10000\n",
      "Episode  290/500 | Reward: -21.0 | Epsilon: 0.125 | Buffer: 10000\n",
      "Episode  291/500 | Reward: -21.0 | Epsilon: 0.122 | Buffer: 10000\n",
      "Episode  292/500 | Reward: -21.0 | Epsilon: 0.120 | Buffer: 10000\n",
      "Episode  293/500 | Reward: -21.0 | Epsilon: 0.117 | Buffer: 10000\n",
      "Episode  294/500 | Reward: -21.0 | Epsilon: 0.114 | Buffer: 10000\n",
      "Episode  295/500 | Reward: -21.0 | Epsilon: 0.111 | Buffer: 10000\n",
      "Episode  296/500 | Reward: -21.0 | Epsilon: 0.109 | Buffer: 10000\n",
      "Episode  297/500 | Reward: -21.0 | Epsilon: 0.106 | Buffer: 10000\n",
      "Episode  298/500 | Reward: -21.0 | Epsilon: 0.103 | Buffer: 10000\n",
      "Episode  299/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  300/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  301/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  302/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  303/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  304/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  305/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  306/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  307/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  308/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  309/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  310/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  311/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  312/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  313/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  314/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  315/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  316/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  317/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  318/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  319/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  320/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  321/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  322/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  323/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  324/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  325/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  326/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  327/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  328/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  329/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  330/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  331/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  332/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  333/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  334/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  335/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  336/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  337/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  338/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  339/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  340/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  341/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  342/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  343/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  344/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  345/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  346/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  347/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  348/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  349/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  350/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  351/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  352/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  353/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  354/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  355/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  356/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  357/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  358/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  359/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  360/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  361/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  362/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  363/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  364/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  365/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  366/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  367/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  368/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  369/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  370/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  371/500 | Reward: -19.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  372/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  373/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  374/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  375/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  376/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  377/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  378/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  379/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  380/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  381/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  382/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  383/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  384/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  385/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  386/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  387/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  388/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  389/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  390/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  391/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  392/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  393/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  394/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  395/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  396/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  397/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  398/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  399/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  400/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  401/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  402/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  403/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  404/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  405/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  406/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  407/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  408/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  409/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  410/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  411/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  412/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  413/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  414/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  415/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  416/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  417/500 | Reward: -19.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  418/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  419/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  420/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  421/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  422/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  423/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  424/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  425/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  426/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  427/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  428/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  429/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  430/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  431/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  432/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  433/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  434/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  435/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  436/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  437/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  438/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  439/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  440/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  441/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  442/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  443/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  444/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  445/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  446/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  447/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  448/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  449/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  450/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  451/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  452/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  453/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  454/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  455/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  456/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  457/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  458/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  459/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  460/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  461/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  462/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  463/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  464/500 | Reward: -21.0 | Epsilon: 0.100 | Buffer: 10000\n",
      "Episode  465/500 | Reward: -20.0 | Epsilon: 0.100 | Buffer: 10000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Full Training Loop (Configurable)\n",
    "# ------------------------------------------------------------\n",
    "# This cell lets we train the DQN for a longer run, e.g., 500+\n",
    "# episodes, with proper logging and target-network updates.\n",
    "# For the assignment, we will likely run multiple sessions and\n",
    "# compare plots.\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training started at:\", datetime.datetime.now())\n",
    "\n",
    "# Reset counters and storage for a fresh training run.\n",
    "steps_done = 0\n",
    "episode_rewards = []\n",
    "\n",
    "# Training configuration\n",
    "num_episodes = NUM_EPISODES         \n",
    "target_update_interval = TARGET_UPDATE_INTERVAL\n",
    "max_steps_per_episode = MAX_STEPS_PER_EPISODE   # safety cutoff to avoid infinite loops\n",
    "\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    frame_stack = FrameStack(num_frames=4)\n",
    "    state = frame_stack.reset(obs)\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step < max_steps_per_episode:\n",
    "        step += 1\n",
    "\n",
    "        # Pick an action based on epsilon-greedy policy.\n",
    "        action, epsilon = select_action(state)\n",
    "\n",
    "        # Interact with the environment.\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update stacked frames → next state\n",
    "        next_state = frame_stack.step(next_obs)\n",
    "\n",
    "        # Save transition\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Move forward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Training step\n",
    "        optimize_model(replay_buffer, use_double_dqn=USE_DOUBLE_DQN)\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    # Sync target network weights at intervals\n",
    "    if episode % target_update_interval == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Episode summary\n",
    "    print(\n",
    "        f\"Episode {episode:4d}/{num_episodes} | \"\n",
    "        f\"Reward: {episode_reward:5.1f} | \"\n",
    "        f\"Epsilon: {get_epsilon(steps_done):.3f} | \"\n",
    "        f\"Buffer: {len(replay_buffer)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7a666cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation episode finished. Total reward: -21.0\n",
      "Number of frames collected: 3056\n"
     ]
    }
   ],
   "source": [
    "def run_evaluation_episode(render_first_frame=True):\n",
    "    \"\"\"\n",
    "    Run a single evaluation episode using a purely greedy policy\n",
    "    (no random exploration) and return the total reward.\n",
    "\n",
    "    Optionally shows the first frame so we know it's working.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the policy network is in eval mode (disables dropout etc., if present).\n",
    "    policy_net.eval()\n",
    "\n",
    "    # Reset the environment and frame stack.\n",
    "    obs, info = env.reset()\n",
    "    frame_stack = FrameStack(num_frames=4)\n",
    "    state = frame_stack.reset(obs)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    frames_eval = []  # store frames here if we want to turn them into a video later\n",
    "\n",
    "    while not done:\n",
    "        # Convert state to a batch of size 1.\n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "\n",
    "        # No epsilon here: purely greedy action selection.\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        # Step the env with the chosen action.\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update frame stack and accumulate reward.\n",
    "        state = frame_stack.step(next_obs)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Save the raw frame for possible video export later.\n",
    "        frames_eval.append(next_obs)\n",
    "\n",
    "    # Optionally show the very first frame of the evaluation run.\n",
    "    if render_first_frame and len(frames_eval) > 0:\n",
    "        plt.imshow(frames_eval[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"First frame of evaluation episode\")\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Evaluation episode finished. Total reward: {total_reward:.1f}\")\n",
    "    print(f\"Number of frames collected: {len(frames_eval)}\")\n",
    "\n",
    "    return total_reward, frames_eval\n",
    "\n",
    "\n",
    "# ---- Quick smoke test: run one evaluation episode ----\n",
    "eval_reward, eval_frames = run_evaluation_episode(render_first_frame=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: pong_evaluation.mp4\n"
     ]
    }
   ],
   "source": [
    "def save_video(frames, filename=\"pong_evaluation.mp4\", fps=30):\n",
    "    \"\"\"\n",
    "    Save a list of RGB frames (H, W, 3) as a video file using OpenCV.\n",
    "\n",
    "    - frames: list of numpy arrays from the environment (RGB).\n",
    "    - filename: output video file name.\n",
    "    - fps: frames per second for the video.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        print(\"No frames to save, skipping video.\")\n",
    "        return\n",
    "\n",
    "    # Get frame height and width from the first frame.\n",
    "    height, width, _ = frames[0].shape\n",
    "\n",
    "    # FourCC code for mp4 output; should work on most setups.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        # OpenCV expects BGR, but Gym gives RGB.\n",
    "        bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video_writer.write(bgr_frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to: {filename}\")\n",
    "\n",
    "\n",
    "# Use the frames from the last evaluation run.\n",
    "save_video(eval_frames, filename=\"pong_evaluation.mp4\", fps=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
